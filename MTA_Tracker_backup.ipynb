{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf185dba",
   "metadata": {},
   "source": [
    "# MTA Tracker - Real-Time Transit Data Analysis\n",
    "\n",
    "This notebook fetches and analyzes data from the MTA (Metropolitan Transportation Authority) API. Follow the steps below to set up your environment and explore the transit data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c54f1d",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "If you're getting \"command not found: python\", you need to install Python or use the correct command. On macOS:\n",
    "\n",
    "### Option A: Using Homebrew (Recommended)\n",
    "```bash\n",
    "# Install Homebrew if not installed\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "# Install Python 3\n",
    "brew install python3\n",
    "\n",
    "# Verify installation\n",
    "python3 --version\n",
    "\n",
    "# Create a virtual environment\n",
    "python3 -m venv mta_env\n",
    "source mta_env/bin/activate\n",
    "\n",
    "# Install required packages\n",
    "pip install requests jupyter pandas matplotlib seaborn protobuf\n",
    "```\n",
    "\n",
    "### Option B: Using Python 3 (if already installed)\n",
    "```bash\n",
    "# Try using python3 instead of python\n",
    "python3 MTA_Tracker.ipynb\n",
    "\n",
    "# Or run Jupyter directly\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### Running This Notebook\n",
    "After setting up, run:\n",
    "```bash\n",
    "jupyter notebook MTA_Tracker.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c16f1",
   "metadata": {},
   "source": [
    "## Quick Fix: Setup with Homebrew\n",
    "\n",
    "If you're having issues, run these commands in your terminal to set up properly:\n",
    "\n",
    "```bash\n",
    "# 1. Install Python 3 with Homebrew\n",
    "brew install python3\n",
    "\n",
    "# 2. Verify Python installation\n",
    "python3 --version\n",
    "\n",
    "# 3. Navigate to your project\n",
    "cd ~/Desktop/Coding\\ Projects/MTATracker\n",
    "\n",
    "# 4. Create a fresh virtual environment\n",
    "python3 -m venv mta_env\n",
    "\n",
    "# 5. Activate the environment\n",
    "source mta_env/bin/activate\n",
    "\n",
    "# 6. Upgrade pip\n",
    "pip install --upgrade pip\n",
    "\n",
    "# 7. Install all required packages at once\n",
    "pip install requests protobuf pandas matplotlib seaborn\n",
    "\n",
    "# 8. Start Jupyter from within the environment\n",
    "jupyter notebook MTA_Tracker.ipynb\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Use `python3` (not `python`)\n",
    "- Always activate your virtual environment before running the notebook\n",
    "- When Jupyter starts, it will use the correct Python from your virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e324ccd",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries\n",
    "\n",
    "We'll use these libraries to fetch, process, and analyze MTA data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cdfbaf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "803b50eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MTATracker class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class MTATracker:\n",
    "    \"\"\"\n",
    "    MTA Tracker - A utility to fetch and process MTA transit data\n",
    "    \n",
    "    This class connects to the Metropolitan Transportation Authority (MTA) real-time\n",
    "    feed and retrieves GTFS-realtime protobuf data. GTFS stands for General Transit Feed\n",
    "    Specification - it's the standard format for transit agency data.\n",
    "    \n",
    "    The MTA provides three types of real-time data:\n",
    "    1. VEHICLE positions (current location and status of buses/trains)\n",
    "    2. TRIP UPDATES (delays and changes to scheduled trips)  \n",
    "    3. ALERTS (service advisories and disruptions)\n",
    "    \n",
    "    All data comes in protobuf (Protocol Buffer) format - a compact binary format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # API Configuration - URL endpoint for MTA's real-time GTFS feed\n",
    "    # The URL contains: gtfs = General Transit Feed Specification\n",
    "    BASE_URL = \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/nyct%2Fgtfs\"\n",
    "    \n",
    "    # TIMEOUT = how long to wait for the API response before giving up (in seconds)\n",
    "    TIMEOUT = 30\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the MTA Tracker.\n",
    "        \n",
    "        This sets up:\n",
    "        - A requests.Session() to reuse the same connection\n",
    "        - last_update: timestamp of when we last fetched data\n",
    "        - data: stores the raw binary protobuf data from the API\n",
    "        \"\"\"\n",
    "        self.session = requests.Session()  # HTTP session to reuse connection\n",
    "        self.last_update = None             # When did we last fetch data?\n",
    "        self.data = None                    # Raw protobuf bytes from API\n",
    "        \n",
    "    def fetch_data(self) -> Optional[bytes]:\n",
    "        \"\"\"\n",
    "        Fetch GTFS data from the MTA API\n",
    "        \n",
    "        Returns:\n",
    "            bytes: Raw response data from the API\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Fetching data from MTA API: {self.BASE_URL}\")\n",
    "            \n",
    "            response = self.session.get(\n",
    "                self.BASE_URL,\n",
    "                timeout=self.TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            self.last_update = datetime.now()\n",
    "            self.data = response.content\n",
    "            \n",
    "            logger.info(f\"Successfully fetched {len(self.data)} bytes of data\")\n",
    "            return self.data\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(\"Request timed out\")\n",
    "            return None\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.error(\"Failed to connect to MTA API\")\n",
    "            return None\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logger.error(f\"HTTP Error: {e.response.status_code}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching data: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_data(self) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Parse the fetched data (currently stores raw bytes)\n",
    "        \n",
    "        Note: MTA GTFS data is in Protocol Buffer format.\n",
    "        Future: Implement protobuf parsing\n",
    "        \n",
    "        Returns:\n",
    "            dict: Parsed data structure\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            logger.warning(\"No data available to parse\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Placeholder for data parsing logic\n",
    "            logger.info(\"Data ready for processing\")\n",
    "            \n",
    "            parsed_data = {\n",
    "                \"raw_bytes\": len(self.data),\n",
    "                \"timestamp\": self.last_update.isoformat() if self.last_update else None,\n",
    "                \"status\": \"fetched\"\n",
    "            }\n",
    "            \n",
    "            return parsed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing data: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the current status of the tracker\n",
    "        \n",
    "        Returns:\n",
    "            dict: Status information\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"last_update\": self.last_update.isoformat() if self.last_update else None,\n",
    "            \"data_available\": self.data is not None,\n",
    "            \"data_size\": len(self.data) if self.data else 0\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the session\"\"\"\n",
    "        self.session.close()\n",
    "        logger.info(\"Session closed\")\n",
    "\n",
    "print(\"‚úì MTATracker class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "030bb816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 20:13:23,824 - INFO - Fetching data from MTA API: https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/nyct%2Fgtfs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Fetching MTA data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 20:13:24,013 - INFO - Successfully fetched 176409 bytes of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully fetched 176409 bytes of data!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tracker\n",
    "tracker = MTATracker()\n",
    "\n",
    "# Fetch data from the API\n",
    "print(\"üîÑ Fetching MTA data...\")\n",
    "data = tracker.fetch_data()\n",
    "\n",
    "if data:\n",
    "    print(f\"‚úì Successfully fetched {len(data)} bytes of data!\")\n",
    "else:\n",
    "    print(\"‚úó Failed to fetch data from MTA API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bedd4ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 20:13:24,018 - INFO - Data ready for processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Tracker Status:\n",
      "{\n",
      "  \"last_update\": \"2026-02-02T20:13:24.013110\",\n",
      "  \"data_available\": true,\n",
      "  \"data_size\": 176409\n",
      "}\n",
      "\n",
      "üìà Parsed Data Info:\n",
      "{\n",
      "  \"raw_bytes\": 176409,\n",
      "  \"timestamp\": \"2026-02-02T20:13:24.013110\",\n",
      "  \"status\": \"fetched\"\n",
      "}\n",
      "\n",
      "üîç First 100 bytes (raw binary):\n",
      "b'\\n{\\n\\x031.0\\x18\\xae\\x97\\x85\\xcc\\x06\\xca>m\\n\\x031.0\\x12\\x0b\\n\\x011\\x12\\x06\\x10\\xae\\x97\\x85\\xcc\\x06\\x12\\x0b\\n\\x012\\x12\\x06\\x10\\xae\\x97\\x85\\xcc\\x06\\x12\\x0b\\n\\x013\\x12\\x06\\x10\\xae\\x97\\x85\\xcc\\x06\\x12\\x0b\\n\\x014\\x12\\x06\\x10\\xae\\x97\\x85\\xcc\\x06\\x12\\x0b\\n\\x015\\x12\\x06\\x10\\xae\\x97\\x85\\xcc\\x06\\x12\\x0b\\n\\x016\\x12\\x06\\x10\\xae\\x97\\x85\\xcc\\x06\\x12'\n"
     ]
    }
   ],
   "source": [
    "# Get tracker status\n",
    "status = tracker.get_status()\n",
    "print(\"üìä Tracker Status:\")\n",
    "print(json.dumps(status, indent=2))\n",
    "\n",
    "# Get parsed data info\n",
    "parsed_info = tracker.parse_data()\n",
    "print(\"\\nüìà Parsed Data Info:\")\n",
    "print(json.dumps(parsed_info, indent=2))\n",
    "\n",
    "# Display first 100 bytes of raw data (it's binary protobuf format)\n",
    "print(f\"\\nüîç First 100 bytes (raw binary):\")\n",
    "print(tracker.data[:100] if tracker.data else \"No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3dc286",
   "metadata": {},
   "source": [
    "## Step 6b: Install GTFS-realtime Packages\n",
    "\n",
    "Before parsing, let's install the necessary packages (including upgrading pip first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a6edba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up environment...\n",
      "\n",
      "1Ô∏è‚É£  Upgrading pip...\n",
      "   ‚úì pip upgraded\n",
      "\n",
      "2Ô∏è‚É£  Installing protobuf...\n",
      "   ‚úì protobuf installed\n",
      "\n",
      "‚úì Environment setup complete!\n",
      "   We'll use pure Python to parse protobuf data.\n"
     ]
    }
   ],
   "source": [
    "# Install only necessary packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Setting up environment...\\n\")\n",
    "\n",
    "# Upgrade pip\n",
    "print(\"1Ô∏è‚É£  Upgrading pip...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', '-q'])\n",
    "    print(\"   ‚úì pip upgraded\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö† pip upgrade skipped: {e}\\n\")\n",
    "\n",
    "# Install protobuf (core requirement)\n",
    "print(\"2Ô∏è‚É£  Installing protobuf...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'protobuf>=3.20', '-q'])\n",
    "    print(\"   ‚úì protobuf installed\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó protobuf install failed: {e}\\n\")\n",
    "    print(\"   This is required for parsing!\\n\")\n",
    "\n",
    "print(\"‚úì Environment setup complete!\")\n",
    "print(\"   We'll use pure Python to parse protobuf data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "84b2915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Parsing MTA GTFS-realtime data...\n",
      "\n",
      "‚úì Successfully parsed MTA feed!\n",
      "\n",
      "üìä Feed Header Information:\n",
      "   - GTFS Version: 1.0\n",
      "   - Incrementality: DIFFERENTIAL\n",
      "\n",
      "üìã Entities: 407 total\n",
      "   - Trip Updates: 0\n",
      "   - Vehicle Positions: 247\n",
      "   - Service Alerts: 159\n",
      "\n",
      "üìç Sample Entities (first 5):\n",
      "   1. ID: 000001..., Type: vehicle\n",
      "   2. ID: 000002..., Type: alert\n",
      "   3. ID: 000003..., Type: vehicle\n",
      "   4. ID: 000004..., Type: alert\n",
      "   5. ID: 000005..., Type: vehicle\n",
      "\n",
      "‚úì Data successfully parsed: 176,409 bytes\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "class ProtobufParser:\n",
    "    \"\"\"Parse GTFS-realtime protobuf without external dependencies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode_varint(data, pos):\n",
    "        \"\"\"Decode protobuf varint\"\"\"\n",
    "        value = 0\n",
    "        shift = 0\n",
    "        while pos < len(data):\n",
    "            byte = data[pos]\n",
    "            pos += 1\n",
    "            value |= (byte & 0x7f) << shift\n",
    "            if (byte & 0x80) == 0:\n",
    "                break\n",
    "            shift += 7\n",
    "        return value, pos\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_feed(data):\n",
    "        \"\"\"Parse FeedMessage\"\"\"\n",
    "        pos = 0\n",
    "        feed = {\"header\": {}, \"entities\": []}\n",
    "        \n",
    "        while pos < len(data):\n",
    "            tag, pos = ProtobufParser.decode_varint(data, pos)\n",
    "            field_num = tag >> 3\n",
    "            wire_type = tag & 0x07\n",
    "            \n",
    "            if wire_type == 2:  # Length-delimited\n",
    "                length, pos = ProtobufParser.decode_varint(data, pos)\n",
    "                field_data = data[pos:pos+length]\n",
    "                pos += length\n",
    "                \n",
    "                if field_num == 1:  # header\n",
    "                    feed[\"header\"] = ProtobufParser.parse_header(field_data)\n",
    "                elif field_num == 2:  # entities\n",
    "                    entity = ProtobufParser.parse_entity(field_data)\n",
    "                    feed[\"entities\"].append(entity)\n",
    "        \n",
    "        return feed\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_header(data):\n",
    "        \"\"\"Parse FeedHeader\"\"\"\n",
    "        header = {}\n",
    "        pos = 0\n",
    "        \n",
    "        while pos < len(data):\n",
    "            tag, pos = ProtobufParser.decode_varint(data, pos)\n",
    "            field_num = tag >> 3\n",
    "            wire_type = tag & 0x07\n",
    "            \n",
    "            if wire_type == 2:  # String\n",
    "                length, pos = ProtobufParser.decode_varint(data, pos)\n",
    "                value = data[pos:pos+length].decode('utf-8', errors='ignore')\n",
    "                pos += length\n",
    "                if field_num == 1:\n",
    "                    header[\"version\"] = value\n",
    "            elif wire_type == 0:  # Varint\n",
    "                value, pos = ProtobufParser.decode_varint(data, pos)\n",
    "                if field_num == 2:\n",
    "                    header[\"timestamp\"] = value\n",
    "                elif field_num == 3:\n",
    "                    header[\"incrementality\"] = value\n",
    "        \n",
    "        return header\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_entity(data):\n",
    "        \"\"\"Parse FeedEntity\"\"\"\n",
    "        entity = {\"id\": \"\", \"type\": \"unknown\"}\n",
    "        pos = 0\n",
    "        \n",
    "        while pos < len(data):\n",
    "            tag, pos = ProtobufParser.decode_varint(data, pos)\n",
    "            field_num = tag >> 3\n",
    "            wire_type = tag & 0x07\n",
    "            \n",
    "            if wire_type == 2:  # Length-delimited\n",
    "                length, pos = ProtobufParser.decode_varint(data, pos)\n",
    "                field_data = data[pos:pos+length]\n",
    "                pos += length\n",
    "                \n",
    "                if field_num == 1:  # id\n",
    "                    entity[\"id\"] = field_data.decode('utf-8', errors='ignore')\n",
    "                elif field_num == 2:  # trip_update\n",
    "                    entity[\"type\"] = \"trip_update\"\n",
    "                elif field_num == 3:  # vehicle\n",
    "                    entity[\"type\"] = \"vehicle\"\n",
    "                elif field_num == 4:  # alert\n",
    "                    entity[\"type\"] = \"alert\"\n",
    "        \n",
    "        return entity\n",
    "\n",
    "# Parse the data\n",
    "if tracker.data:\n",
    "    print(\"üîÑ Parsing MTA GTFS-realtime data...\\n\")\n",
    "    \n",
    "    try:\n",
    "        feed = ProtobufParser.parse_feed(tracker.data)\n",
    "        \n",
    "        print(\"‚úì Successfully parsed MTA feed!\\n\")\n",
    "        \n",
    "        # Display header info\n",
    "        print(\"üìä Feed Header Information:\")\n",
    "        if feed[\"header\"]:\n",
    "            if \"version\" in feed[\"header\"]:\n",
    "                print(f\"   - GTFS Version: {feed['header']['version']}\")\n",
    "            if \"timestamp\" in feed[\"header\"]:\n",
    "                from datetime import datetime\n",
    "                ts = feed[\"header\"][\"timestamp\"]\n",
    "                try:\n",
    "                    readable = datetime.fromtimestamp(ts)\n",
    "                    print(f\"   - Last Update: {readable}\")\n",
    "                except:\n",
    "                    print(f\"   - Timestamp (Unix): {ts}\")\n",
    "            if \"incrementality\" in feed[\"header\"]:\n",
    "                inc_type = \"FULL_DATASET\" if feed[\"header\"][\"incrementality\"] == 0 else \"DIFFERENTIAL\"\n",
    "                print(f\"   - Incrementality: {inc_type}\")\n",
    "        \n",
    "        # Count entity types\n",
    "        trip_updates = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"trip_update\")\n",
    "        vehicles = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"vehicle\")\n",
    "        alerts = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"alert\")\n",
    "        \n",
    "        print(f\"\\nüìã Entities: {len(feed['entities'])} total\")\n",
    "        print(f\"   - Trip Updates: {trip_updates}\")\n",
    "        print(f\"   - Vehicle Positions: {vehicles}\")\n",
    "        print(f\"   - Service Alerts: {alerts}\")\n",
    "        \n",
    "        # Show samples\n",
    "        if feed[\"entities\"]:\n",
    "            print(f\"\\nüìç Sample Entities (first 5):\")\n",
    "            for i, e in enumerate(feed[\"entities\"][:5]):\n",
    "                print(f\"   {i+1}. ID: {e['id'][:20]}..., Type: {e['type']}\")\n",
    "        \n",
    "        print(f\"\\n‚úì Data successfully parsed: {len(tracker.data):,} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error parsing: {e}\")\n",
    "        print(f\"   Data size: {len(tracker.data):,} bytes\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö† No data available. Run the 'Fetch Data from MTA API' cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a3417928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEBUGGING: Analyzing protobuf structure\n",
      "\n",
      "Entity #1: (89 bytes)\n",
      "  Raw hex (first 50 bytes): 0a063030303030311a4f0a340a0e3131353535305f312e2e4e3033521a0832303236303230322a0131ca3e140a1030312031\n",
      "  Field tags found: [(1, 2), (3, 2)]\n",
      "\n",
      "Entity #2: (78 bytes)\n",
      "  Raw hex (first 50 bytes): 0a0630303030303222440a340a0e3131353535305f312e2e4e3033521a0832303236303230322a0131ca3e140a1030312031\n",
      "  Field tags found: [(1, 2), (4, 2)]\n",
      "\n",
      "Entity #3: (119 bytes)\n",
      "  Raw hex (first 50 bytes): 0a063030303030331a6d0a340a0e3131353935305f312e2e4e3033521a0832303236303230322a0131ca3e140a1030312031\n",
      "  Field tags found: [(1, 2), (3, 2)]\n",
      "\n",
      "Entity #4: (78 bytes)\n",
      "  Raw hex (first 50 bytes): 0a0630303030303422440a340a0e3131353935305f312e2e4e3033521a0832303236303230322a0131ca3e140a1030312031\n",
      "  Field tags found: [(1, 2), (4, 2)]\n",
      "\n",
      "Entity #5: (91 bytes)\n",
      "  Raw hex (first 50 bytes): 0a063030303030351a510a360a0e3131363030305f312e2e533033521a0832303236303230322a0131ca3e160a1030312031\n",
      "  Field tags found: [(1, 2), (3, 2)]\n",
      "\n",
      "Field number reference:\n",
      "  1 = id (string)\n",
      "  2 = trip_update (nested message)\n",
      "  3 = vehicle (nested message)\n",
      "  4 = alert (nested message)\n"
     ]
    }
   ],
   "source": [
    "# Debug: Inspect raw protobuf structure\n",
    "print(\"üîç DEBUGGING: Analyzing protobuf structure\\n\")\n",
    "\n",
    "# Let's manually parse a few entities and inspect their binary content\n",
    "data = tracker.data\n",
    "pos = 0\n",
    "entity_count = 0\n",
    "detailed_entities = []\n",
    "\n",
    "while pos < len(data) and entity_count < 5:  # Just check first 5 entities\n",
    "    tag, pos = ProtobufParser.decode_varint(data, pos)\n",
    "    field_num = tag >> 3\n",
    "    wire_type = tag & 0x07\n",
    "    \n",
    "    if wire_type == 2:  # Length-delimited (our main field)\n",
    "        length, pos = ProtobufParser.decode_varint(data, pos)\n",
    "        field_data = data[pos:pos+length]\n",
    "        pos += length\n",
    "        \n",
    "        if field_num == 2:  # entities\n",
    "            entity_count += 1\n",
    "            \n",
    "            print(f\"Entity #{entity_count}: ({len(field_data)} bytes)\")\n",
    "            print(f\"  Raw hex (first 50 bytes): {field_data[:50].hex()}\")\n",
    "            \n",
    "            # Parse all fields in this entity\n",
    "            field_tags = []\n",
    "            temp_pos = 0\n",
    "            while temp_pos < len(field_data):\n",
    "                try:\n",
    "                    tag2, temp_pos = ProtobufParser.decode_varint(field_data, temp_pos)\n",
    "                    field_num2 = tag2 >> 3\n",
    "                    wire_type2 = tag2 & 0x07\n",
    "                    field_tags.append((field_num2, wire_type2))\n",
    "                    \n",
    "                    if wire_type2 == 0:  # varint\n",
    "                        val, temp_pos = ProtobufParser.decode_varint(field_data, temp_pos)\n",
    "                    elif wire_type2 == 2:  # length-delimited\n",
    "                        length2, temp_pos = ProtobufParser.decode_varint(field_data, temp_pos)\n",
    "                        temp_pos += length2\n",
    "                    elif wire_type2 == 5:  # 32-bit\n",
    "                        temp_pos += 4\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "            print(f\"  Field tags found: {field_tags}\")\n",
    "            print()\n",
    "\n",
    "print(\"Field number reference:\")\n",
    "print(\"  1 = id (string)\")\n",
    "print(\"  2 = trip_update (nested message)\")\n",
    "print(\"  3 = vehicle (nested message)\")\n",
    "print(\"  4 = alert (nested message)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5a983082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEEP DIVE: Parsing nested message content\n",
      "\n",
      "=== Entity #1 ===\n",
      "  ID: 000001\n",
      "  Type: vehicle\n",
      "  Nested message size: 79 bytes\n",
      "  Nested hex: 0a340a0e3131353535305f312e2e4e3033521a0832303236303230322a0131ca3e140a1030312031\n",
      "  Nested fields: ['(1,2)', '(2,2)']\n",
      "\n",
      "=== Entity #2 ===\n",
      "  ID: 000002\n",
      "  Type: alert\n",
      "  Nested message size: 68 bytes\n",
      "  Nested hex: 0a340a0e3131353535305f312e2e4e3033521a0832303236303230322a0131ca3e140a1030312031\n",
      "  Nested fields: ['(1,2)', '(3,0)', '(5,0)', '(7,2)']\n",
      "\n",
      "=== Entity #3 ===\n",
      "  ID: 000003\n",
      "  Type: vehicle\n",
      "  Nested message size: 109 bytes\n",
      "  Nested hex: 0a340a0e3131353935305f312e2e4e3033521a0832303236303230322a0131ca3e140a1030312031\n",
      "  Nested fields: ['(1,2)', '(2,2)', '(2,2)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into nested message structure\n",
    "print(\"üîç DEEP DIVE: Parsing nested message content\\n\")\n",
    "\n",
    "data = tracker.data\n",
    "pos = 0\n",
    "entity_num = 0\n",
    "\n",
    "while pos < len(data) and entity_num < 3:\n",
    "    tag, pos = ProtobufParser.decode_varint(data, pos)\n",
    "    field_num = tag >> 3\n",
    "    wire_type = tag & 0x07\n",
    "    \n",
    "    if wire_type == 2:\n",
    "        length, pos = ProtobufParser.decode_varint(data, pos)\n",
    "        field_data = data[pos:pos+length]\n",
    "        pos += length\n",
    "        \n",
    "        if field_num == 2:  # entities\n",
    "            entity_num += 1\n",
    "            print(f\"=== Entity #{entity_num} ===\")\n",
    "            \n",
    "            # Parse this entity\n",
    "            ep = 0\n",
    "            while ep < len(field_data):\n",
    "                tag2, ep = ProtobufParser.decode_varint(field_data, ep)\n",
    "                field_num2 = tag2 >> 3\n",
    "                wire_type2 = tag2 & 0x07\n",
    "                \n",
    "                if wire_type2 == 2:  # length-delimited (strings and nested messages)\n",
    "                    length2, ep = ProtobufParser.decode_varint(field_data, ep)\n",
    "                    content = field_data[ep:ep+length2]\n",
    "                    ep += length2\n",
    "                    \n",
    "                    if field_num2 == 1:  # id\n",
    "                        print(f\"  ID: {content.decode('utf-8', errors='ignore')}\")\n",
    "                    elif field_num2 in [2, 3, 4]:  # trip_update, vehicle, or alert\n",
    "                        entity_type = ['trip_update', 'vehicle', 'alert'][field_num2-2]\n",
    "                        print(f\"  Type: {entity_type}\")\n",
    "                        print(f\"  Nested message size: {len(content)} bytes\")\n",
    "                        print(f\"  Nested hex: {content[:40].hex()}\")\n",
    "                        \n",
    "                        # Parse nested message fields\n",
    "                        np = 0\n",
    "                        nested_fields = []\n",
    "                        while np < len(content):\n",
    "                            try:\n",
    "                                tag3, np = ProtobufParser.decode_varint(content, np)\n",
    "                                field_num3 = tag3 >> 3\n",
    "                                wire_type3 = tag3 & 0x07\n",
    "                                nested_fields.append(f\"({field_num3},{wire_type3})\")\n",
    "                                \n",
    "                                if wire_type3 == 0:\n",
    "                                    val, np = ProtobufParser.decode_varint(content, np)\n",
    "                                elif wire_type3 == 2:\n",
    "                                    len3, np = ProtobufParser.decode_varint(content, np)\n",
    "                                    np += len3\n",
    "                                elif wire_type3 == 5:\n",
    "                                    np += 4\n",
    "                            except:\n",
    "                                break\n",
    "                        \n",
    "                        print(f\"  Nested fields: {nested_fields}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2fc6dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Better parser loaded!\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED PARSER - Focuses on extracting key data (route_id, trip_id)\n",
    "class BetterProtobufParser:\n",
    "    \"\"\"Simplified parser that extracts entity ID, type, and route/trip information\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode_varint(data, pos):\n",
    "        \"\"\"Decode protobuf varint\"\"\"\n",
    "        value = 0\n",
    "        shift = 0\n",
    "        while pos < len(data):\n",
    "            byte = data[pos]\n",
    "            pos += 1\n",
    "            value |= (byte & 0x7f) << shift\n",
    "            if (byte & 0x80) == 0:\n",
    "                break\n",
    "            shift += 7\n",
    "        return value, pos\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_feed(data):\n",
    "        \"\"\"\n",
    "        Parse the top-level FeedMessage from raw protobuf bytes.\n",
    "        \n",
    "        Returns a dictionary with:\n",
    "        - header: {\"version\": \"1.0\", \"timestamp\": ...}\n",
    "        - entities: [list of all vehicles, trip_updates, and alerts]\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Read field tags from the data stream\n",
    "        2. Extract field_num (what field is this?) and wire_type (what format?)\n",
    "        3. If it's a length-delimited field (type 2), read its contents\n",
    "        4. Route to appropriate parser based on field_num\n",
    "        \"\"\"\n",
    "        current_position = 0\n",
    "        feed_dictionary = {\"header\": {}, \"entities\": []}\n",
    "        \n",
    "        while current_position < len(data):\n",
    "            tag, current_position = BetterProtobufParser.decode_varint(data, current_position)\n",
    "            field_number = tag >> 3       # Get field number (upper bits)\n",
    "            wire_format_type = tag & 0x07 # Get wire type (lower 3 bits)\n",
    "            \n",
    "            if wire_format_type == 2:  # Length-delimited\n",
    "                length, current_position = BetterProtobufParser.decode_varint(data, current_position)\n",
    "                field_data = data[current_position:current_position+length]\n",
    "                current_position += length\n",
    "                \n",
    "                if field_number == 1:  # header\n",
    "                    feed_dictionary[\"header\"] = BetterProtobufParser.parse_header(field_data)\n",
    "                elif field_number == 2:  # entities\n",
    "                    entity = BetterProtobufParser.parse_entity(field_data)\n",
    "                    feed_dictionary[\"entities\"].append(entity)\n",
    "        \n",
    "        return feed_dictionary\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_header(data):\n",
    "        \"\"\"Parse FeedHeader\"\"\"\n",
    "        header = {}\n",
    "        pos = 0\n",
    "        \n",
    "        while pos < len(data):\n",
    "            tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            field_num = tag >> 3\n",
    "            wire_type = tag & 0x07\n",
    "            \n",
    "            if wire_type == 2:\n",
    "                length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                value = data[pos:pos+length].decode('utf-8', errors='ignore')\n",
    "                pos += length\n",
    "                if field_num == 1:\n",
    "                    header[\"version\"] = value\n",
    "            elif wire_type == 0:\n",
    "                value, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                if field_num == 2:\n",
    "                    header[\"timestamp\"] = value\n",
    "                elif field_num == 3:\n",
    "                    header[\"incrementality\"] = value\n",
    "        \n",
    "        return header\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_entity(data):\n",
    "        \"\"\"Parse FeedEntity and extract core data\"\"\"\n",
    "        entity = {\n",
    "            \"id\": \"\",\n",
    "            \"type\": \"unknown\",\n",
    "            \"trip_id\": \"N/A\",\n",
    "            \"route_id\": \"N/A\",\n",
    "            \"delay_seconds\": \"N/A\",\n",
    "            \"alert_message\": \"N/A\",\n",
    "            \"affected_routes\": \"N/A\"\n",
    "        }\n",
    "        pos = 0\n",
    "        \n",
    "        while pos < len(data):\n",
    "            tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            field_num = tag >> 3\n",
    "            wire_type = tag & 0x07\n",
    "            \n",
    "            if wire_type == 2:  # Length-delimited\n",
    "                length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                field_data = data[pos:pos+length]\n",
    "                pos += length\n",
    "                \n",
    "                if field_num == 1:  # id\n",
    "                    entity[\"id\"] = field_data.decode('utf-8', errors='ignore')\n",
    "                elif field_num == 2:  # trip_update\n",
    "                    entity[\"type\"] = \"trip_update\"\n",
    "                    BetterProtobufParser.extract_trip_update_info(field_data, entity)\n",
    "                elif field_num == 3:  # vehicle\n",
    "                    entity[\"type\"] = \"vehicle\"\n",
    "                    BetterProtobufParser.extract_vehicle_info(field_data, entity)\n",
    "                elif field_num == 4:  # alert\n",
    "                    entity[\"type\"] = \"alert\"\n",
    "                    BetterProtobufParser.extract_alert_info(field_data, entity)\n",
    "        \n",
    "        return entity\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_trip_update_info(data, entity):\n",
    "        \"\"\"Extract route_id, trip_id, and delay from trip_update\"\"\"\n",
    "        pos = 0\n",
    "        while pos < len(data):\n",
    "            try:\n",
    "                tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                field_num = tag >> 3\n",
    "                wire_type = tag & 0x07\n",
    "                \n",
    "                if wire_type == 2:  # Length-delimited\n",
    "                    length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                    field_data = data[pos:pos+length]\n",
    "                    pos += length\n",
    "                    \n",
    "                    if field_num == 1:  # trip (nested message)\n",
    "                        BetterProtobufParser.extract_trip_info(field_data, entity)\n",
    "                    elif field_num == 2:  # stop_time_updates\n",
    "                        pass  # Skip for now\n",
    "                elif wire_type == 0:  # Varint\n",
    "                    value, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                    if field_num == 3:  # delay\n",
    "                        entity[\"delay_seconds\"] = str(value)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_vehicle_info(data, entity):\n",
    "        \"\"\"Extract route_id and trip_id from vehicle\"\"\"\n",
    "        pos = 0\n",
    "        while pos < len(data):\n",
    "            try:\n",
    "                tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                field_num = tag >> 3\n",
    "                wire_type = tag & 0x07\n",
    "                \n",
    "                if wire_type == 2:  # Length-delimited\n",
    "                    length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                    field_data = data[pos:pos+length]\n",
    "                    pos += length\n",
    "                    \n",
    "                    if field_num == 1:  # trip (nested message)\n",
    "                        BetterProtobufParser.extract_trip_info(field_data, entity)\n",
    "                    elif field_num == 2:  # position\n",
    "                        pass  # Skip GPS data for now\n",
    "                elif wire_type == 0:  # Varint\n",
    "                    value, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_trip_info(data, entity):\n",
    "        \"\"\"Extract trip_id and route_id from TripDescriptor\"\"\"\n",
    "        pos = 0\n",
    "        while pos < len(data):\n",
    "            try:\n",
    "                tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                field_num = tag >> 3\n",
    "                wire_type = tag & 0x07\n",
    "                \n",
    "                if wire_type == 2:  # String\n",
    "                    length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                    value = data[pos:pos+length].decode('utf-8', errors='ignore')\n",
    "                    pos += length\n",
    "                    \n",
    "                    if field_num == 1:  # trip_id\n",
    "                        entity[\"trip_id\"] = value\n",
    "                    elif field_num == 3:  # route_id\n",
    "                        entity[\"route_id\"] = value\n",
    "                elif wire_type == 0:  # Varint\n",
    "                    value, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_alert_info(data, entity):\n",
    "        \"\"\"Extract alert message and affected routes from Alert\"\"\"\n",
    "        pos = 0\n",
    "        route_identifier = \"N/A\"\n",
    "        \n",
    "        while pos < len(data):\n",
    "            try:\n",
    "                tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                field_num = tag >> 3\n",
    "                wire_type = tag & 0x07\n",
    "                \n",
    "                if wire_type == 2:  # Length-delimited\n",
    "                    length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "                    field_data = data[pos:pos+length]\n",
    "                    pos += length\n",
    "                    \n",
    "                    if field_num == 7:  # description_text - actually contains route identifier\n",
    "                        route_identifier = field_data.decode('utf-8', errors='ignore').strip()\n",
    "                elif wire_type == 0:  # Varint\n",
    "                    value, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # Set the affected routes to the route identifier found in field 7\n",
    "        if route_identifier != \"N/A\":\n",
    "            entity[\"affected_routes\"] = route_identifier\n",
    "            entity[\"alert_message\"] = f\"Service Alert\"  # Generic alert message since MTA doesn't provide text\n",
    "\n",
    "print(\"‚úì Better parser loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "74148ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing improved parser...\n",
      "\n",
      "‚úì Successfully parsed!\n",
      "\n",
      "üìä Feed Header:\n",
      "   Version: 1.0\n",
      "\n",
      "üìã Entities: 407 total\n",
      "   - Trip Updates: 0\n",
      "   - Vehicles: 247\n",
      "   - Alerts: 159\n",
      "\n",
      "üìå Sample Entities (first 10):\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      " 1. ID: 000001          | Type: vehicle      | Route: 20260202   | Trip: 115550_1..N03R       | Delay: N/A       \n",
      " 2. ID: 000002          | Type: alert        | Route: N/A        | Trip: N/A                  | Delay: N/A       \n",
      " 3. ID: 000003          | Type: vehicle      | Route: 20260202   | Trip: 115950_1..N03R       | Delay: N/A       \n",
      " 4. ID: 000004          | Type: alert        | Route: N/A        | Trip: N/A                  | Delay: N/A       \n",
      " 5. ID: 000005          | Type: vehicle      | Route: 20260202   | Trip: 116000_1..S03R       | Delay: N/A       \n",
      " 6. ID: 000006          | Type: alert        | Route: N/A        | Trip: N/A                  | Delay: N/A       \n",
      " 7. ID: 000007          | Type: vehicle      | Route: 20260202   | Trip: 116300_1..N03R       | Delay: N/A       \n",
      " 8. ID: 000008          | Type: alert        | Route: N/A        | Trip: N/A                  | Delay: N/A       \n",
      " 9. ID: 000009          | Type: vehicle      | Route: 20260202   | Trip: 116400_1..S03R       | Delay: N/A       \n",
      "10. ID: 000010          | Type: alert        | Route: N/A        | Trip: N/A                  | Delay: N/A       \n",
      "\n",
      "‚úì Data extraction complete!\n",
      "   - Entities with route_id: 247\n",
      "   - Entities with trip_id: 247\n"
     ]
    }
   ],
   "source": [
    "# Test the new parser\n",
    "print(\"üîÑ Testing improved parser...\\n\")\n",
    "\n",
    "if tracker.data:\n",
    "    try:\n",
    "        feed = BetterProtobufParser.parse_feed(tracker.data)\n",
    "        \n",
    "        print(\"‚úì Successfully parsed!\\n\")\n",
    "        \n",
    "        # Display header\n",
    "        print(\"üìä Feed Header:\")\n",
    "        if feed[\"header\"]:\n",
    "            print(f\"   Version: {feed['header'].get('version', 'N/A')}\")\n",
    "            ts = feed[\"header\"].get('timestamp', 'N/A')\n",
    "            if ts != 'N/A':\n",
    "                from datetime import datetime\n",
    "                readable = datetime.fromtimestamp(ts)\n",
    "                print(f\"   Timestamp: {readable}\")\n",
    "        \n",
    "        # Count entity types\n",
    "        trip_updates = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"trip_update\")\n",
    "        vehicles = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"vehicle\")\n",
    "        alerts = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"alert\")\n",
    "        \n",
    "        print(f\"\\nüìã Entities: {len(feed['entities'])} total\")\n",
    "        print(f\"   - Trip Updates: {trip_updates}\")\n",
    "        print(f\"   - Vehicles: {vehicles}\")\n",
    "        print(f\"   - Alerts: {alerts}\")\n",
    "        \n",
    "        # Show first 10 with data\n",
    "        print(f\"\\nüìå Sample Entities (first 10):\")\n",
    "        print(\"-\" * 130)\n",
    "        for i, e in enumerate(feed[\"entities\"][:10]):\n",
    "            trip = e['trip_id'][:20] if e['trip_id'] != 'N/A' else 'N/A'\n",
    "            route = e['route_id'][:10] if e['route_id'] != 'N/A' else 'N/A'\n",
    "            delay = e['delay_seconds'][:10] if e['delay_seconds'] != 'N/A' else 'N/A'\n",
    "            print(f\"{i+1:2}. ID: {e['id'][:15]:15} | Type: {e['type']:12} | Route: {route:10} | Trip: {trip:20} | Delay: {delay:10}\")\n",
    "        \n",
    "        # Count populated fields\n",
    "        routes_found = sum(1 for e in feed['entities'] if e['route_id'] != 'N/A')\n",
    "        trips_found = sum(1 for e in feed['entities'] if e['trip_id'] != 'N/A')\n",
    "        \n",
    "        print(f\"\\n‚úì Data extraction complete!\")\n",
    "        print(f\"   - Entities with route_id: {routes_found}\")\n",
    "        print(f\"   - Entities with trip_id: {trips_found}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fc0bee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Re-exporting MTA data with extracted fields...\n",
      "\n",
      "üìÅ Saving to: logs/20260202_201324/\n",
      "\n",
      "‚úì Saved: mta_feed_20260202_201324_fixed.json (407 entities)\n",
      "‚úì Saved: mta_entities_20260202_201324_fixed.csv\n",
      "‚úì Saved: mta_metadata_20260202_201324_fixed.txt\n",
      "\n",
      "üìÅ ALL FILES SAVED TO: logs/20260202_201324/\n",
      "   1. mta_feed_20260202_201324_fixed.json\n",
      "   2. mta_entities_20260202_201324_fixed.csv\n",
      "   3. mta_metadata_20260202_201324_fixed.txt\n",
      "\n",
      "‚úÖ Export complete!\n",
      "   - Alerts with messages: 159\n"
     ]
    }
   ],
   "source": [
    "# Re-export data using the new parser with actual extracted data - Save to logs folder\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üíæ Re-exporting MTA data with extracted fields...\\n\")\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "logs_dir = Path(\"logs\")\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create timestamped subdirectory for this run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = logs_dir / timestamp\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Saving to: logs/{timestamp}/\\n\")\n",
    "\n",
    "if tracker.data and 'feed' in locals():\n",
    "    # 1. Save full feed as JSON\n",
    "    json_file = run_dir / f\"mta_feed_{timestamp}_fixed.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(feed, f, indent=2, default=str)\n",
    "    print(f\"‚úì Saved: {json_file.name} ({len(feed['entities'])} entities)\")\n",
    "    \n",
    "    # 2. Save entities as CSV with ACTUAL extracted data\n",
    "    csv_file = run_dir / f\"mta_entities_{timestamp}_fixed.csv\"\n",
    "    if feed[\"entities\"]:\n",
    "        with open(csv_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Entity_ID\", \"Type\", \"Route_ID\", \"Trip_ID\", \"Delay_Seconds\", \"Alert_Message\", \"Affected_Routes\"])\n",
    "            \n",
    "            for entity in feed[\"entities\"]:\n",
    "                entity_id = entity.get(\"id\", \"N/A\")[:50]\n",
    "                ent_type = entity.get(\"type\", \"unknown\")\n",
    "                route = entity.get(\"route_id\", \"N/A\")\n",
    "                trip = entity.get(\"trip_id\", \"N/A\")\n",
    "                delay = entity.get(\"delay_seconds\", \"N/A\")\n",
    "                alert_msg = entity.get(\"alert_message\", \"N/A\")\n",
    "                affected_routes = entity.get(\"affected_routes\", \"N/A\")\n",
    "                \n",
    "                writer.writerow([entity_id, ent_type, route, trip, delay, alert_msg, affected_routes])\n",
    "    \n",
    "    print(f\"‚úì Saved: {csv_file.name}\")\n",
    "    \n",
    "    # 3. Save metadata with updated legend\n",
    "    meta_file = run_dir / f\"mta_metadata_{timestamp}_fixed.txt\"\n",
    "    with open(meta_file, 'w') as f:\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(\"MTA GTFS-REALTIME DATA EXPORT (IMPROVED PARSER)\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"EXPORT TIMESTAMP: \" + datetime.now().isoformat() + \"\\n\")\n",
    "        f.write(\"DATA SIZE: \" + f\"{len(tracker.data):,} bytes\\n\\n\")\n",
    "        \n",
    "        f.write(\"FEED HEADER INFORMATION:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        if feed[\"header\"]:\n",
    "            f.write(f\"GTFS Version: {feed['header'].get('version', 'N/A')}\\n\")\n",
    "            ts = feed['header'].get('timestamp', 'N/A')\n",
    "            if ts != 'N/A':\n",
    "                readable = datetime.fromtimestamp(ts)\n",
    "                f.write(f\"Feed Timestamp: {readable}\\n\")\n",
    "        \n",
    "        f.write(\"\\nENTITY BREAKDOWN:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        trip_updates = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"trip_update\")\n",
    "        vehicles = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"vehicle\")\n",
    "        alerts = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"alert\")\n",
    "        f.write(f\"Total Entities: {len(feed['entities'])}\\n\")\n",
    "        f.write(f\"  - Trip Updates: {trip_updates}\\n\")\n",
    "        f.write(f\"  - Vehicles: {vehicles}\\n\")\n",
    "        f.write(f\"  - Alerts: {alerts}\\n\\n\")\n",
    "        \n",
    "        # Data extraction stats\n",
    "        routes_found = sum(1 for e in feed['entities'] if e['route_id'] != 'N/A')\n",
    "        trips_found = sum(1 for e in feed['entities'] if e['trip_id'] != 'N/A')\n",
    "        delays_found = sum(1 for e in feed['entities'] if e['delay_seconds'] != 'N/A')\n",
    "        \n",
    "        f.write(\"DATA EXTRACTION STATISTICS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"Entities with route_id: {routes_found} ({100*routes_found/len(feed['entities']):.1f}%)\\n\")\n",
    "        f.write(f\"Entities with trip_id: {trips_found} ({100*trips_found/len(feed['entities']):.1f}%)\\n\")\n",
    "        f.write(f\"Entities with delay: {delays_found} ({100*delays_found/len(feed['entities']):.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(\"DATA LEGEND / FIELD REFERENCE\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"FILE DESCRIPTIONS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"1. {json_file}\\n\")\n",
    "        f.write(\"   - Full structured JSON export\\n\")\n",
    "        f.write(\"   - Contains all entity data and metadata\\n\\n\")\n",
    "        \n",
    "        f.write(f\"2. {csv_file}\\n\")\n",
    "        f.write(\"   - Comma-separated values for Excel/Sheets\\n\")\n",
    "        f.write(\"   - Easy to analyze and filter\\n\\n\")\n",
    "        \n",
    "        f.write(\"FIELD DEFINITIONS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"Entity_ID:     Unique identifier for this transit entity\\n\")\n",
    "        f.write(\"Type:          Entity type: 'vehicle', 'trip_update', or 'alert'\\n\")\n",
    "        f.write(\"Route_ID:      MTA route identifier (e.g., '1', 'A', 'F', etc.)\\n\")\n",
    "        f.write(\"Trip_ID:       Unique identifier for the specific trip\\n\")\n",
    "        f.write(\"Delay_Seconds: Delay in seconds (N/A for vehicles/alerts)\\n\\n\")\n",
    "        \n",
    "        f.write(\"ENTITY TYPES:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"vehicle:\\n\")\n",
    "        f.write(\"  - Real-time vehicle location and status\\n\")\n",
    "        f.write(\"  - Contains: route_id, trip_id, position, bearing\\n\")\n",
    "        f.write(\"  - Count: {}\\n\\n\".format(vehicles))\n",
    "        \n",
    "        f.write(\"trip_update:\\n\")\n",
    "        f.write(\"  - Real-time updates to scheduled trips\\n\")\n",
    "        f.write(\"  - Contains: route_id, trip_id, delay, stop updates\\n\")\n",
    "        f.write(\"  - Count: {}\\n\\n\".format(trip_updates))\n",
    "        \n",
    "        f.write(\"alert:\\n\")\n",
    "        f.write(\"  - Service alerts and announcements\\n\")\n",
    "        f.write(\"  - Contains: alert message, affected routes/agencies\\n\")\n",
    "        f.write(\"  - Count: {}\\n\\n\".format(alerts))\n",
    "        \n",
    "        f.write(\"DATA VALUE LEGEND:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"N/A = Data not available for this entity\\n\\n\")\n",
    "        \n",
    "        f.write(\"CSV FIELD DEFINITIONS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"Entity_ID:        Unique identifier for this transit entity\\n\")\n",
    "        f.write(\"Type:             Entity type: 'vehicle', 'trip_update', or 'alert'\\n\")\n",
    "        f.write(\"Route_ID:         MTA route identifier (e.g., '1', 'A', 'F')\\n\")\n",
    "        f.write(\"Trip_ID:          Unique identifier for the specific trip\\n\")\n",
    "        f.write(\"Delay_Seconds:    Delay in seconds (for trip_update entities)\\n\")\n",
    "        f.write(\"Alert_Message:    Alert header and description (for alert entities)\\n\")\n",
    "        f.write(\"Affected_Routes:  Routes/agencies affected by alert\\n\\n\")\n",
    "        \n",
    "        f.write(\"HOW TO USE THIS DATA:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"1. Open CSV file in Excel/Google Sheets\\n\")\n",
    "        f.write(\"2. Filter by Type = 'alert' to see all service alerts\\n\")\n",
    "        f.write(\"3. Sort by Route_ID to see specific lines\\n\")\n",
    "        f.write(\"4. Use Alert_Message to understand service issues\\n\")\n",
    "        f.write(\"5. Use Affected_Routes to see which routes are impacted\\n\")\n",
    "        \n",
    "    \n",
    "    print(f\"‚úì Saved: {meta_file.name}\")\n",
    "    print(f\"\\nüìÅ ALL FILES SAVED TO: logs/{timestamp}/\")\n",
    "    print(f\"   1. {json_file.name}\")\n",
    "    print(f\"   2. {csv_file.name}\")\n",
    "    print(f\"   3. {meta_file.name}\")\n",
    "    \n",
    "    # Count alerts\n",
    "    alerts_with_data = sum(1 for e in feed['entities'] if e['type'] == 'alert' and e['alert_message'] != 'N/A')\n",
    "    print(f\"\\n‚úÖ Export complete!\")\n",
    "    print(f\"   - Alerts with messages: {alerts_with_data}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö† No parsed data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5fcbf19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîß CLEAN DATA PROCESSING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "üìÇ Reading: logs/20260202_201324/mta_entities_20260202_201324_fixed.csv\n",
      "\n",
      "1Ô∏è‚É£  Parsing vehicles...\n",
      "   ‚úì Extracted 247 unique trains\n",
      "\n",
      "2Ô∏è‚É£  Parsing alerts...\n",
      "   ‚úì Extracted 139 unique alert codes\n",
      "\n",
      "3Ô∏è‚É£  Matching alerts to trains...\n",
      "   ‚úì Attached 5387 alerts to 232 trains\n",
      "\n",
      "4Ô∏è‚É£  Building JSON response...\n",
      "   ‚úì Response built with 247 trains\n",
      "\n",
      "5Ô∏è‚É£  Saved: logs/20260202_201324/app_data.json\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PIPELINE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä Statistics:\n",
      "   Total trains: 247\n",
      "   Trains on schedule: 15\n",
      "   Trains affected by alerts: 232\n",
      "   Total active alerts: 139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === REFACTORED: CLEAN PARSING & ALERT MATCHING ===\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "# === CONSTANTS ===\n",
    "\n",
    "# Trip ID Format: HHMMSS_LINE..DIRECTIONPATTERN\n",
    "# Example: 113350_1..S03R ‚Üí Line 1, Southbound, started at 11:33:50\n",
    "DIRECTION_MAP = {\n",
    "    \"S\": \"Southbound\",\n",
    "    \"N\": \"Northbound\"\n",
    "}\n",
    "\n",
    "# Alert Route Code Format: [LINES][DIRECTION]\n",
    "# Example: 139S ‚Üí Lines 1,3,9 going Southbound\n",
    "ALERT_CODE_DIRECTION_SUFFIX = {\"S\", \"N\"}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß CLEAN DATA PROCESSING PIPELINE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: HELPER FUNCTIONS - PARSE TRIP ID\n",
    "# ============================================================================\n",
    "\n",
    "def extract_line_from_trip(trip_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract subway line from trip ID.\n",
    "    \n",
    "    Trip format: HHMMSS_LINE..DIRECTIONPATTERN\n",
    "    Example: 113350_1..S03R ‚Üí \"1\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        line_and_direction = trip_id.split(\"_\")[1]\n",
    "        return line_and_direction[0]  # First character is always the line\n",
    "    except (IndexError, AttributeError):\n",
    "        return \"N/A\"\n",
    "\n",
    "def extract_direction_from_trip(trip_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract direction from trip ID (normalized to full name).\n",
    "    \n",
    "    Trip format: HHMMSS_LINE..DIRECTIONPATTERN\n",
    "    Example: 113350_1..S03R ‚Üí \"Southbound\" (S ‚Üí Southbound)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        line_and_direction = trip_id.split(\"_\")[1]\n",
    "        # S or N appears in the string\n",
    "        direction_char = \"S\" if \"S\" in line_and_direction else \"N\"\n",
    "        return DIRECTION_MAP.get(direction_char, \"Unknown\")\n",
    "    except (IndexError, AttributeError):\n",
    "        return \"Unknown\"\n",
    "\n",
    "def extract_start_time_from_trip(trip_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract start time from trip ID.\n",
    "    \n",
    "    Trip format: HHMMSS_LINE..DIRECTIONPATTERN\n",
    "    Example: 113350_1..S03R ‚Üí \"113350\" (11:33:50)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return trip_id.split(\"_\")[0]\n",
    "    except (IndexError, AttributeError):\n",
    "        return \"N/A\"\n",
    "\n",
    "def extract_delay_minutes(delay_seconds_str: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert delay from seconds string to minutes float.\n",
    "    Returns None if not available or invalid.\n",
    "    \"\"\"\n",
    "    if delay_seconds_str == \"N/A\" or not delay_seconds_str:\n",
    "        return None\n",
    "    try:\n",
    "        return int(delay_seconds_str) / 60\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: HELPER FUNCTIONS - PARSE ALERT CODES\n",
    "# ============================================================================\n",
    "\n",
    "def extract_lines_from_alert_code(code: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract affected line numbers from alert code.\n",
    "    \n",
    "    Alert format: [LINES][DIRECTION]\n",
    "    Example: 139S ‚Üí [\"1\", \"3\", \"9\"] (ignoring duplicates/trailing zeros)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove direction letter (S or N) from end\n",
    "        lines_str = code[:-1]\n",
    "        # Extract individual line numbers, remove duplicates, sort\n",
    "        lines = sorted(set(lines_str))\n",
    "        return lines\n",
    "    except (IndexError, TypeError):\n",
    "        return []\n",
    "\n",
    "def extract_direction_from_alert_code(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract direction from alert code (normalized).\n",
    "    \n",
    "    Alert format: [LINES][DIRECTION]\n",
    "    Example: 139S ‚Üí \"Southbound\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        direction_char = code[-1]  # Last character is S or N\n",
    "        return DIRECTION_MAP.get(direction_char, \"Unknown\")\n",
    "    except (IndexError, TypeError):\n",
    "        return \"Unknown\"\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: PARSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def parse_vehicles_from_csv(csv_file: Path) -> Dict[tuple, Dict]:\n",
    "    \"\"\"\n",
    "    Parse vehicle rows from CSV into structured objects.\n",
    "    \n",
    "    Key: (line, direction, start_time) ‚Üí unique train\n",
    "    Returns: Dictionary of train objects ready for alert attachment\n",
    "    \"\"\"\n",
    "    trains = {}\n",
    "    \n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['Type'] != 'vehicle':\n",
    "                continue\n",
    "            \n",
    "            trip_id = row.get('Trip_ID', 'N/A')\n",
    "            if trip_id == 'N/A':\n",
    "                continue\n",
    "            \n",
    "            # Extract trip components\n",
    "            line = extract_line_from_trip(trip_id)\n",
    "            direction = extract_direction_from_trip(trip_id)\n",
    "            start_time = extract_start_time_from_trip(trip_id)\n",
    "            delay_minutes = extract_delay_minutes(row.get('Delay_Seconds', 'N/A'))\n",
    "            \n",
    "            # Use composite key to avoid duplicates\n",
    "            train_key = (line, direction, start_time)\n",
    "            \n",
    "            # Create train object (only if key not already seen)\n",
    "            if train_key not in trains:\n",
    "                trains[train_key] = {\n",
    "                    \"line\": line,\n",
    "                    \"direction\": direction,\n",
    "                    \"start_time\": start_time,\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"delay_minutes\": delay_minutes,\n",
    "                    \"alerts\": set()  # Use set to avoid duplicate alerts\n",
    "                }\n",
    "    \n",
    "    return trains\n",
    "\n",
    "def parse_alerts_from_csv(csv_file: Path) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Parse alert rows from CSV into structured objects.\n",
    "    \n",
    "    Key: alert code (e.g., \"139S\") ‚Üí alert object\n",
    "    Returns: Dictionary of alert objects\n",
    "    \"\"\"\n",
    "    alerts = {}\n",
    "    \n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['Type'] != 'alert':\n",
    "                continue\n",
    "            \n",
    "            affected_routes = row.get('Affected_Routes', 'N/A')\n",
    "            alert_message = row.get('Alert_Message', 'N/A')\n",
    "            \n",
    "            # Skip invalid alerts\n",
    "            if affected_routes == 'N/A' or alert_message == 'N/A':\n",
    "                continue\n",
    "            \n",
    "            # Parse alert structure\n",
    "            lines = extract_lines_from_alert_code(affected_routes)\n",
    "            direction = extract_direction_from_alert_code(affected_routes)\n",
    "            \n",
    "            # Skip if parsing failed\n",
    "            if not lines or direction == \"Unknown\":\n",
    "                continue\n",
    "            \n",
    "            # Store by affected_routes code (unique identifier)\n",
    "            alerts[affected_routes] = {\n",
    "                \"lines\": lines,\n",
    "                \"direction\": direction,\n",
    "                \"message\": alert_message,\n",
    "                \"affected_routes_code\": affected_routes\n",
    "            }\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: MATCHING LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "def attach_alerts_to_trains(trains: Dict, alerts: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Attach alerts to trains that match line + direction.\n",
    "    \n",
    "    Returns: Total number of alert attachments made\n",
    "    \"\"\"\n",
    "    alert_count = 0\n",
    "    \n",
    "    # Index alerts by (line, direction) for faster lookup\n",
    "    alerts_by_line_direction = defaultdict(list)\n",
    "    for alert in alerts.values():\n",
    "        direction = alert[\"direction\"]\n",
    "        for line in alert[\"lines\"]:\n",
    "            alerts_by_line_direction[(line, direction)].append(alert)\n",
    "    \n",
    "    # Attach alerts to trains\n",
    "    for train in trains.values():\n",
    "        line = train[\"line\"]\n",
    "        direction = train[\"direction\"]\n",
    "        \n",
    "        # Lookup alerts for this (line, direction)\n",
    "        matching_alerts = alerts_by_line_direction.get((line, direction), [])\n",
    "        \n",
    "        for alert in matching_alerts:\n",
    "            train[\"alerts\"].add(alert[\"message\"])\n",
    "            alert_count += 1\n",
    "    \n",
    "    return alert_count\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: OUTPUT FORMATTING\n",
    "# ============================================================================\n",
    "\n",
    "def convert_to_json_ready(trains: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convert trains from processing format to JSON-ready format.\n",
    "    Converts sets to lists, removes internal keys.\n",
    "    \"\"\"\n",
    "    trains_list = []\n",
    "    \n",
    "    for train in trains.values():\n",
    "        trains_list.append({\n",
    "            \"line\": train[\"line\"],\n",
    "            \"direction\": train[\"direction\"],\n",
    "            \"start_time\": train[\"start_time\"],\n",
    "            \"trip_id\": train[\"trip_id\"],\n",
    "            \"delay_minutes\": train[\"delay_minutes\"],\n",
    "            \"alerts\": list(train[\"alerts\"]) if train[\"alerts\"] else []\n",
    "        })\n",
    "    \n",
    "    return trains_list\n",
    "\n",
    "def build_response_json(trains_list: List[Dict], alerts: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Build final JSON response with summary statistics.\n",
    "    \"\"\"\n",
    "    trains_with_alerts = sum(1 for t in trains_list if t[\"alerts\"])\n",
    "    \n",
    "    return {\n",
    "        \"last_updated\": __import__('datetime').datetime.now().isoformat(),\n",
    "        \"trains\": trains_list,\n",
    "        \"alerts\": list(alerts.values()),\n",
    "        \"summary\": {\n",
    "            \"total_trains\": len(trains_list),\n",
    "            \"total_unique_alerts\": len(alerts),\n",
    "            \"trains_with_alerts\": trains_with_alerts,\n",
    "            \"trains_on_schedule\": len(trains_list) - trains_with_alerts\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "# Find latest log directory\n",
    "logs_dir = Path(\"logs\")\n",
    "latest_dir = sorted(logs_dir.glob(\"*\"))[-1]\n",
    "csv_file = sorted(Path(latest_dir).glob(\"mta_entities_*_fixed.csv\"))[0]\n",
    "\n",
    "print(f\"üìÇ Reading: {csv_file}\\n\")\n",
    "\n",
    "# Step 1: Parse vehicles\n",
    "print(\"1Ô∏è‚É£  Parsing vehicles...\")\n",
    "trains = parse_vehicles_from_csv(csv_file)\n",
    "print(f\"   ‚úì Extracted {len(trains)} unique trains\\n\")\n",
    "\n",
    "# Step 2: Parse alerts\n",
    "print(\"2Ô∏è‚É£  Parsing alerts...\")\n",
    "alerts = parse_alerts_from_csv(csv_file)\n",
    "print(f\"   ‚úì Extracted {len(alerts)} unique alert codes\\n\")\n",
    "\n",
    "# Step 3: Attach alerts to trains\n",
    "print(\"3Ô∏è‚É£  Matching alerts to trains...\")\n",
    "total_attachments = attach_alerts_to_trains(trains, alerts)\n",
    "trains_with_alerts = sum(1 for t in trains.values() if t[\"alerts\"])\n",
    "print(f\"   ‚úì Attached {total_attachments} alerts to {trains_with_alerts} trains\\n\")\n",
    "\n",
    "# Step 4: Build JSON output\n",
    "print(\"4Ô∏è‚É£  Building JSON response...\")\n",
    "trains_list = convert_to_json_ready(trains)\n",
    "response = build_response_json(trains_list, alerts)\n",
    "print(f\"   ‚úì Response built with {len(response['trains'])} trains\\n\")\n",
    "\n",
    "# Step 5: Save to file\n",
    "app_data_file = latest_dir / \"app_data.json\"\n",
    "with open(app_data_file, 'w') as f:\n",
    "    json.dump(response, f, indent=2)\n",
    "print(f\"5Ô∏è‚É£  Saved: {app_data_file}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "üìä Statistics:\n",
    "   Total trains: {response['summary']['total_trains']}\n",
    "   Trains on schedule: {response['summary']['trains_on_schedule']}\n",
    "   Trains affected by alerts: {response['summary']['trains_with_alerts']}\n",
    "   Total active alerts: {response['summary']['total_unique_alerts']}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6519c653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ CLEAN QUERY INTERFACE\n",
      "================================================================================\n",
      "\n",
      "‚úÖ MTADataService ready\n",
      "\n",
      "Available methods:\n",
      "  ‚Ä¢ service.get_line_status(line)\n",
      "  ‚Ä¢ service.get_trains_by_direction(line, direction)\n",
      "  ‚Ä¢ service.get_delayed_trains()\n",
      "  ‚Ä¢ service.get_trains_with_alerts()\n",
      "  ‚Ä¢ service.get_train_by_trip_id(trip_id)\n",
      "  ‚Ä¢ service.search_alert(text)\n",
      "\n",
      "================================================================================\n",
      "üìã EXAMPLE QUERIES\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  Line 1 Status:\n",
      "   Total trains: 40\n",
      "   With alerts: 40\n",
      "   Active alerts: 70\n",
      "\n",
      "2Ô∏è‚É£  Line 1 Southbound Trains:\n",
      "   Count: 18\n",
      "   Sample: 116000 (On schedule) - 1 alerts\n",
      "\n",
      "3Ô∏è‚É£  All Delayed Trains:\n",
      "   Total: 0\n",
      "\n",
      "4Ô∏è‚É£  Trains Affected by Alerts:\n",
      "   Total: 232\n",
      "   ‚Ä¢ Line 1 Northbound (115550)\n",
      "      1 alert(s)\n",
      "   ‚Ä¢ Line 1 Northbound (115950)\n",
      "      1 alert(s)\n",
      "\n",
      "5Ô∏è‚É£  Search Alerts for 'Service Alert':\n",
      "   Found: 139 matches\n",
      "\n",
      "================================================================================\n",
      "‚úÖ REFACTORED CODE COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === PART 7: CLEAN API FOR APP QUERIES ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ CLEAN QUERY INTERFACE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "class MTADataService:\n",
    "    \"\"\"\n",
    "    Clean interface for querying train and alert data.\n",
    "    Encapsulates all business logic in focused methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, response: Dict):\n",
    "        self.response = response\n",
    "        self.trains = response[\"trains\"]\n",
    "        self.alerts = response[\"alerts\"]\n",
    "        # Build indexes for fast lookups\n",
    "        self._build_indexes()\n",
    "    \n",
    "    def _build_indexes(self):\n",
    "        \"\"\"Build lookup indexes for O(1) queries\"\"\"\n",
    "        # Index trains by line\n",
    "        self.trains_by_line = defaultdict(list)\n",
    "        for train in self.trains:\n",
    "            self.trains_by_line[train[\"line\"]].append(train)\n",
    "        \n",
    "        # Index trains by (line, direction)\n",
    "        self.trains_by_line_direction = defaultdict(list)\n",
    "        for train in self.trains:\n",
    "            key = (train[\"line\"], train[\"direction\"])\n",
    "            self.trains_by_line_direction[key].append(train)\n",
    "        \n",
    "        # Index alerts by line\n",
    "        self.alerts_by_line = defaultdict(list)\n",
    "        for alert in self.alerts:\n",
    "            for line in alert[\"lines\"]:\n",
    "                self.alerts_by_line[line].append(alert)\n",
    "    \n",
    "    def get_line_status(self, line: str) -> Dict:\n",
    "        \"\"\"Get complete status of a subway line\"\"\"\n",
    "        trains = self.trains_by_line.get(line, [])\n",
    "        alerts = self.alerts_by_line.get(line, [])\n",
    "        \n",
    "        return {\n",
    "            \"line\": line,\n",
    "            \"total_trains\": len(trains),\n",
    "            \"trains_with_alerts\": sum(1 for t in trains if t[\"alerts\"]),\n",
    "            \"trains\": trains,\n",
    "            \"alerts\": alerts\n",
    "        }\n",
    "    \n",
    "    def get_trains_by_direction(self, line: str, direction: str) -> List[Dict]:\n",
    "        \"\"\"Get trains on a specific line and direction\"\"\"\n",
    "        key = (line, direction)\n",
    "        return self.trains_by_line_direction.get(key, [])\n",
    "    \n",
    "    def get_delayed_trains(self) -> List[Dict]:\n",
    "        \"\"\"Get all trains currently delayed\"\"\"\n",
    "        return [t for t in self.trains if t[\"delay_minutes\"] and t[\"delay_minutes\"] > 0]\n",
    "    \n",
    "    def get_trains_with_alerts(self) -> List[Dict]:\n",
    "        \"\"\"Get all trains currently affected by alerts\"\"\"\n",
    "        return [t for t in self.trains if t[\"alerts\"]]\n",
    "    \n",
    "    def get_train_by_trip_id(self, trip_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Lookup single train by trip ID\"\"\"\n",
    "        for train in self.trains:\n",
    "            if train[\"trip_id\"] == trip_id:\n",
    "                return train\n",
    "        return None\n",
    "    \n",
    "    def search_alert(self, search_term: str) -> List[Dict]:\n",
    "        \"\"\"Search alerts by text\"\"\"\n",
    "        return [\n",
    "            a for a in self.alerts\n",
    "            if search_term.lower() in a[\"message\"].lower()\n",
    "        ]\n",
    "\n",
    "# Create service instance\n",
    "service = MTADataService(response)\n",
    "\n",
    "print(\"‚úÖ MTADataService ready\\n\")\n",
    "print(\"Available methods:\")\n",
    "print(\"  ‚Ä¢ service.get_line_status(line)\")\n",
    "print(\"  ‚Ä¢ service.get_trains_by_direction(line, direction)\")\n",
    "print(\"  ‚Ä¢ service.get_delayed_trains()\")\n",
    "print(\"  ‚Ä¢ service.get_trains_with_alerts()\")\n",
    "print(\"  ‚Ä¢ service.get_train_by_trip_id(trip_id)\")\n",
    "print(\"  ‚Ä¢ service.search_alert(text)\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE QUERIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã EXAMPLE QUERIES\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Query 1: Line Status\n",
    "print(\"1Ô∏è‚É£  Line 1 Status:\")\n",
    "line_1_status = service.get_line_status(\"1\")\n",
    "print(f\"   Total trains: {line_1_status['total_trains']}\")\n",
    "print(f\"   With alerts: {line_1_status['trains_with_alerts']}\")\n",
    "print(f\"   Active alerts: {len(line_1_status['alerts'])}\\n\")\n",
    "\n",
    "# Query 2: Specific direction\n",
    "print(\"2Ô∏è‚É£  Line 1 Southbound Trains:\")\n",
    "line_1_south = service.get_trains_by_direction(\"1\", \"Southbound\")\n",
    "print(f\"   Count: {len(line_1_south)}\")\n",
    "if line_1_south:\n",
    "    sample = line_1_south[0]\n",
    "    delay_str = f\" (Delayed {sample['delay_minutes']:.0f}min)\" if sample['delay_minutes'] else \"(On schedule)\"\n",
    "    alert_str = f\" - {len(sample['alerts'])} alerts\" if sample['alerts'] else \"\"\n",
    "    print(f\"   Sample: {sample['start_time']} {delay_str}{alert_str}\\n\")\n",
    "\n",
    "# Query 3: Delayed trains\n",
    "print(\"3Ô∏è‚É£  All Delayed Trains:\")\n",
    "delayed = service.get_delayed_trains()\n",
    "print(f\"   Total: {len(delayed)}\")\n",
    "if delayed:\n",
    "    for train in delayed[:2]:\n",
    "        print(f\"   ‚Ä¢ Line {train['line']} {train['direction']}: {train['delay_minutes']:.1f} min late\")\n",
    "print()\n",
    "\n",
    "# Query 4: Trains with alerts\n",
    "print(\"4Ô∏è‚É£  Trains Affected by Alerts:\")\n",
    "affected = service.get_trains_with_alerts()\n",
    "print(f\"   Total: {len(affected)}\")\n",
    "if affected:\n",
    "    for train in affected[:2]:\n",
    "        print(f\"   ‚Ä¢ Line {train['line']} {train['direction']} ({train['start_time']})\")\n",
    "        print(f\"      {len(train['alerts'])} alert(s)\")\n",
    "print()\n",
    "\n",
    "# Query 5: Search\n",
    "print(\"5Ô∏è‚É£  Search Alerts for 'Service Alert':\")\n",
    "search_results = service.search_alert(\"Service Alert\")\n",
    "print(f\"   Found: {len(search_results)} matches\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ REFACTORED CODE COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e720fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚ú® FORMATTING FOR RIDERS\n",
      "================================================================================\n",
      "\n",
      "üì± Sample Rider-Friendly Trains:\n",
      "\n",
      "1. Line 1 Northbound\n",
      "   Departure: 11:55 AM\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service alert affecting Northbound 1 trains\n",
      "\n",
      "2. Line 1 Northbound\n",
      "   Departure: 11:59 AM\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service alert affecting Northbound 1 trains\n",
      "\n",
      "3. Line 1 Southbound\n",
      "   Departure: 12:00 PM\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service alert affecting Southbound 1 trains\n",
      "\n",
      "4. Line 1 Northbound\n",
      "   Departure: 12:03 PM\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service alert affecting Northbound 1 trains\n",
      "\n",
      "5. Line 1 Southbound\n",
      "   Departure: 12:04 PM\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service alert affecting Southbound 1 trains\n",
      "\n",
      "‚úÖ Saved rider-friendly version: logs/20260202_201324/app_data_rider.json\n",
      "\n",
      "================================================================================\n",
      "üìä BACKEND vs RIDER-FRIENDLY FORMAT\n",
      "================================================================================\n",
      "\n",
      "BACKEND FORMAT (raw):\n",
      "{\n",
      "  \"line\": \"1\",\n",
      "  \"direction\": \"Northbound\",\n",
      "  \"start_time\": \"115550\",\n",
      "  \"trip_id\": \"115550_1..N03R\",\n",
      "  \"delay_minutes\": null,\n",
      "  \"alerts\": [\n",
      "    \"Service Alert\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "RIDER-FRIENDLY FORMAT:\n",
      "{\n",
      "  \"line\": \"1\",\n",
      "  \"direction\": \"Northbound\",\n",
      "  \"start_time\": \"11:55 AM\",\n",
      "  \"delay_minutes\": null,\n",
      "  \"alerts\": [\n",
      "    \"Service alert affecting Northbound 1 trains\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "‚ú® KEY DIFFERENCES:\n",
      "   ‚úì Time: 113350 ‚Üí 11:33 AM (readable)\n",
      "   ‚úì Hidden: trip_id (backend detail)\n",
      "   ‚úì Alerts: Generic ‚Üí Specific (with line/direction)\n",
      "   ‚úì No null delays: Either shown or omitted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === RIDER-FRIENDLY OUTPUT FORMATTING ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ú® FORMATTING FOR RIDERS\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPERS: FORMAT DATA FOR RIDERS\n",
    "# ============================================================================\n",
    "\n",
    "def format_24hr_to_12hr(time_24hr: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert GTFS HHMMSS (possibly overflow) into 12-hour clock time.\n",
    "\n",
    "    Examples:\n",
    "    \"113350\" ‚Üí \"11:33 AM\"\n",
    "    \"116000\" ‚Üí \"12:00 PM\"   (minute overflow)\n",
    "    \"246300\" ‚Üí \"1:03 AM\"    (hour + minute overflow)\n",
    "    \"\"\"\n",
    "    if not time_24hr or not time_24hr.isdigit():\n",
    "        return time_24hr\n",
    "\n",
    "    # Pad to 6 digits if needed\n",
    "    time_24hr = time_24hr.zfill(6)\n",
    "\n",
    "    try:\n",
    "        raw_hour = int(time_24hr[:-4])      # everything except MMSS\n",
    "        raw_minute = int(time_24hr[-4:-2])  # middle two\n",
    "        raw_second = int(time_24hr[-2:])    # last two\n",
    "\n",
    "        # Convert entire thing to seconds to normalize overflow\n",
    "        total_seconds = raw_hour * 3600 + raw_minute * 60 + raw_second\n",
    "\n",
    "        # Wrap around after 24 hours\n",
    "        total_seconds %= 24 * 3600\n",
    "\n",
    "        hour = total_seconds // 3600\n",
    "        minute = (total_seconds % 3600) // 60\n",
    "\n",
    "        am_pm = \"AM\" if hour < 12 else \"PM\"\n",
    "        hour_12 = hour % 12 or 12\n",
    "\n",
    "        return f\"{hour_12}:{minute:02d} {am_pm}\"\n",
    "\n",
    "    except ValueError:\n",
    "        return time_24hr\n",
    "\n",
    "\n",
    "def enhance_alert_message(alert_message: str, line: str, direction: str) -> str:\n",
    "    \"\"\"\n",
    "    Expand generic alert with line and direction context.\n",
    "    \n",
    "    Input: \"Service Alert\", line=\"1\", direction=\"Northbound\"\n",
    "    Output: \"Service alert affecting Northbound 1 trains\"\n",
    "    \"\"\"\n",
    "    if not alert_message or alert_message == \"N/A\":\n",
    "        return \"Service unavailable\"\n",
    "    \n",
    "    # If it's generic, enhance with line/direction\n",
    "    if alert_message.lower() == \"service alert\":\n",
    "        return f\"Service alert affecting {direction} {line} trains\"\n",
    "    \n",
    "    # Otherwise return as-is (assume already descriptive)\n",
    "    return alert_message\n",
    "\n",
    "def format_train_for_rider(train: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Transform backend train object into rider-friendly format.\n",
    "    \n",
    "    Removes: trip_id (confusing), keeps: line, direction, readable time\n",
    "    Enhances: alert messages with context\n",
    "    \"\"\"\n",
    "    formatted_alerts = [\n",
    "        enhance_alert_message(alert, train[\"line\"], train[\"direction\"])\n",
    "        for alert in train[\"alerts\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"line\": train[\"line\"],\n",
    "        \"direction\": train[\"direction\"],\n",
    "        \"start_time\": format_24hr_to_12hr(train[\"start_time\"]),\n",
    "        \"delay_minutes\": train[\"delay_minutes\"],\n",
    "        \"alerts\": formatted_alerts\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD RIDER-FRIENDLY RESPONSE\n",
    "# ============================================================================\n",
    "\n",
    "def build_rider_response(response: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Transform backend response into rider-friendly format.\n",
    "    \"\"\"\n",
    "    # Format each train for riders\n",
    "    formatted_trains = [\n",
    "        format_train_for_rider(train)\n",
    "        for train in response[\"trains\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"last_updated\": response[\"last_updated\"],\n",
    "        \"trains\": formatted_trains,\n",
    "        \"summary\": response[\"summary\"]\n",
    "    }\n",
    "\n",
    "# Build rider-friendly response\n",
    "rider_response = build_rider_response(response)\n",
    "\n",
    "print(\"üì± Sample Rider-Friendly Trains:\\n\")\n",
    "\n",
    "# Show examples\n",
    "for i, train in enumerate(rider_response[\"trains\"][:5]):\n",
    "    print(f\"{i+1}. Line {train['line']} {train['direction']}\")\n",
    "    print(f\"   Departure: {train['start_time']}\")\n",
    "    if train['delay_minutes']:\n",
    "        print(f\"   ‚è±Ô∏è  {train['delay_minutes']:.0f} minutes late\")\n",
    "    else:\n",
    "        print(f\"   ‚úì On schedule\")\n",
    "    if train['alerts']:\n",
    "        for alert in train['alerts']:\n",
    "            print(f\"   ‚ö†Ô∏è  {alert}\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RIDER-FRIENDLY VERSION\n",
    "# ============================================================================\n",
    "\n",
    "rider_json_file = latest_dir / \"app_data_rider.json\"\n",
    "with open(rider_json_file, 'w') as f:\n",
    "    json.dump(rider_response, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved rider-friendly version: {rider_json_file}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SHOW JSON COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä BACKEND vs RIDER-FRIENDLY FORMAT\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"BACKEND FORMAT (raw):\")\n",
    "print(json.dumps(response[\"trains\"][0], indent=2))\n",
    "print()\n",
    "\n",
    "print(\"RIDER-FRIENDLY FORMAT:\")\n",
    "print(json.dumps(rider_response[\"trains\"][0], indent=2))\n",
    "print()\n",
    "\n",
    "print(\"‚ú® KEY DIFFERENCES:\")\n",
    "print(\"   ‚úì Time: 113350 ‚Üí 11:33 AM (readable)\")\n",
    "print(\"   ‚úì Hidden: trip_id (backend detail)\")\n",
    "print(\"   ‚úì Alerts: Generic ‚Üí Specific (with line/direction)\")\n",
    "print(\"   ‚úì No null delays: Either shown or omitted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0e6533fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîß BUILDING APP DATA STRUCTURES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Created MTAData object\n",
      "   - 247 trains loaded\n",
      "   - 139 active alerts\n",
      "\n",
      "üì° EXAMPLE API CALLS FOR YOUR APP\n",
      "\n",
      "1Ô∏è‚É£  Get all trains on Line 1\n",
      "   Found 40 trains on Line 1\n",
      "   - Northbound: 22 trains\n",
      "   - Southbound: 18 trains\n",
      "\n",
      "2Ô∏è‚É£  Get alerts affecting Line 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[182]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Example 2: Get alerts for a line\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m2Ô∏è‚É£  Get alerts affecting Line 1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m line_1_alerts = \u001b[43mmta_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_line_alerts\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m line_1_alerts:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m alert \u001b[38;5;129;01min\u001b[39;00m line_1_alerts[:\u001b[32m3\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[182]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mMTAData.get_line_alerts\u001b[39m\u001b[34m(self, line_num)\u001b[39m\n\u001b[32m     34\u001b[39m result = []\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m alert \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.alerts:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m line_num \u001b[38;5;129;01min\u001b[39;00m \u001b[43malert\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlines\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m     37\u001b[39m         result.append(alert)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# === BUILD TRAIN DATA STRUCTURE FOR YOUR APP ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß BUILDING APP DATA STRUCTURES\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Create a structure that's optimized for your app\n",
    "class MTAData:\n",
    "    \"\"\"Stores parsed MTA data in app-friendly format\"\"\"\n",
    "    \n",
    "    def __init__(self, trains_dict, alerts_list):\n",
    "        self.trains = trains_dict\n",
    "        self.alerts = alerts_list\n",
    "        self.last_updated = datetime.now().isoformat()\n",
    "    \n",
    "    def get_line(self, line_num):\n",
    "        \"\"\"Get all trains on a specific line\"\"\"\n",
    "        result = []\n",
    "        for train in self.trains.values():\n",
    "            if train['line'] == line_num:\n",
    "                result.append(train)\n",
    "        return result\n",
    "    \n",
    "    def get_line_direction(self, line_num, direction):\n",
    "        \"\"\"Get trains on a specific line/direction\"\"\"\n",
    "        result = []\n",
    "        for train in self.trains.values():\n",
    "            if train['line'] == line_num and train['direction'] == direction:\n",
    "                result.append(train)\n",
    "        return result\n",
    "    \n",
    "    def get_line_alerts(self, line_num):\n",
    "        \"\"\"Get all active alerts affecting a line\"\"\"\n",
    "        result = []\n",
    "        for alert in self.alerts:\n",
    "            if line_num in alert['lines']:\n",
    "                result.append(alert)\n",
    "        return result\n",
    "    \n",
    "    def to_json(self, file_path=None):\n",
    "        \"\"\"Export data as JSON\"\"\"\n",
    "        data = {\n",
    "            \"last_updated\": self.last_updated,\n",
    "            \"trains\": list(self.trains.values()),\n",
    "            \"alerts\": self.alerts,\n",
    "            \"summary\": {\n",
    "                \"total_trains\": len(self.trains),\n",
    "                \"total_alerts\": len(self.alerts),\n",
    "                \"trains_with_delays\": sum(1 for t in self.trains.values() if t['delay_minutes']),\n",
    "                \"trains_with_alerts\": sum(1 for t in self.trains.values() if t['alerts'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if file_path:\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            print(f\"‚úì Saved to {file_path}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Create the data object\n",
    "mta_data = MTAData(trains, alerts)\n",
    "\n",
    "print(f\"‚úÖ Created MTAData object\")\n",
    "print(f\"   - {len(mta_data.trains)} trains loaded\")\n",
    "print(f\"   - {len(mta_data.alerts)} active alerts\\n\")\n",
    "\n",
    "# === EXAMPLE API CALLS ===\n",
    "\n",
    "print(\"üì° EXAMPLE API CALLS FOR YOUR APP\\n\")\n",
    "\n",
    "# Example 1: Get status of Line 1\n",
    "print(\"1Ô∏è‚É£  Get all trains on Line 1\")\n",
    "line_1_trains = mta_data.get_line('1')\n",
    "print(f\"   Found {len(line_1_trains)} trains on Line 1\")\n",
    "northbound_1 = [t for t in line_1_trains if t['direction'] == 'Northbound']\n",
    "southbound_1 = [t for t in line_1_trains if t['direction'] == 'Southbound']\n",
    "print(f\"   - Northbound: {len(northbound_1)} trains\")\n",
    "print(f\"   - Southbound: {len(southbound_1)} trains\\n\")\n",
    "\n",
    "# Example 2: Get alerts for a line\n",
    "print(\"2Ô∏è‚É£  Get alerts affecting Line 1\")\n",
    "line_1_alerts = mta_data.get_line_alerts('1')\n",
    "if line_1_alerts:\n",
    "    for alert in line_1_alerts[:3]:\n",
    "        print(f\"   ‚ö†Ô∏è  {alert['direction']}: {alert['affected_routes_code']} - {alert['message']}\")\n",
    "else:\n",
    "    print(\"   ‚úì No active alerts on Line 1\")\n",
    "print()\n",
    "\n",
    "# Example 3: Get specific train\n",
    "print(\"3Ô∏è‚É£  Get trains on Line 6 going Northbound\")\n",
    "line_6_north = mta_data.get_line_direction('6', 'Northbound')\n",
    "if line_6_north:\n",
    "    for train in line_6_north[:2]:\n",
    "        status = \"üî¥ DELAYED\" if train['delay_minutes'] else \"‚úì On schedule\"\n",
    "        print(f\"   {train['start_time']} - {status}\")\n",
    "        if train['alerts']:\n",
    "            for alert in train['alerts']:\n",
    "                print(f\"      ‚ö†Ô∏è  {alert}\")\n",
    "else:\n",
    "    print(\"   No trains found\")\n",
    "print()\n",
    "\n",
    "# === EXPORT FOR YOUR APP ===\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üíæ EXPORTING DATA FOR YOUR APPLICATION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Save as JSON for app consumption\n",
    "import datetime\n",
    "app_data_file = latest_dir / \"app_data.json\"\n",
    "mta_data.to_json(app_data_file)\n",
    "\n",
    "print(f\"\\n‚úÖ Ready for app development!\")\n",
    "print(f\"   Use mta_data.get_line() and mta_data.get_line_alerts()\")\n",
    "print(f\"   to fetch data in your frontend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c76db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì± EXAMPLE: RIDER APP INTERFACE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUBWAY LINE 1\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  ACTIVE ALERTS ON THIS LINE:\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "\n",
      "NORTHBOUND: 21 trains\n",
      "\n",
      "üöÜ 1 Train ‚Äì Northbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 113950)\n",
      "\n",
      "üöÜ 1 Train ‚Äì Northbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 114350)\n",
      "\n",
      "   ... and 19 more northbound trains\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "SOUTHBOUND: 20 trains\n",
      "\n",
      "üöÜ 1 Train ‚Äì Southbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 113350)\n",
      "\n",
      "üöÜ 1 Train ‚Äì Southbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 113750)\n",
      "\n",
      "   ... and 18 more southbound trains\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUBWAY LINE 6\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  ACTIVE ALERTS ON THIS LINE:\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Northbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "   ‚Ä¢ Southbound: Service Alert\n",
      "\n",
      "NORTHBOUND: 30 trains\n",
      "\n",
      "üöÜ 6 Train ‚Äì Northbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 083900)\n",
      "\n",
      "üöÜ 6 Train ‚Äì Northbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 112250)\n",
      "\n",
      "   ... and 28 more northbound trains\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "SOUTHBOUND: 19 trains\n",
      "\n",
      "üöÜ 6 Train ‚Äì Southbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 112750)\n",
      "\n",
      "üöÜ 6 Train ‚Äì Southbound\n",
      "   ‚úì On schedule\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   ‚ö†Ô∏è  Service Alert\n",
      "   (Started: 113550)\n",
      "\n",
      "   ... and 17 more southbound trains\n",
      "\n",
      "================================================================================\n",
      "‚úÖ App data structure ready for development!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === EXAMPLE: Build a simple app interface ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üì± EXAMPLE: RIDER APP INTERFACE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def format_train_status(train):\n",
    "    \"\"\"Format a single train for display to rider\"\"\"\n",
    "    status = \"\"\n",
    "    \n",
    "    # Line and direction\n",
    "    status += f\"üöÜ {train['line']} Train ‚Äì {train['direction']}\\n\"\n",
    "    \n",
    "    # Delay info\n",
    "    if train['delay_minutes'] and train['delay_minutes'] > 0:\n",
    "        status += f\"   ‚è±Ô∏è  {train['delay_minutes']:.0f} minutes late\\n\"\n",
    "    else:\n",
    "        status += f\"   ‚úì On schedule\\n\"\n",
    "    \n",
    "    # Alerts\n",
    "    if train['alerts']:\n",
    "        for alert in train['alerts']:\n",
    "            status += f\"   ‚ö†Ô∏è  {alert}\\n\"\n",
    "    \n",
    "    status += f\"   (Started: {train['start_time']})\"\n",
    "    \n",
    "    return status\n",
    "\n",
    "def show_line_status(line_num):\n",
    "    \"\"\"Show status of all trains on a line\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"SUBWAY LINE {line_num}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    # Get line alerts\n",
    "    line_alerts = mta_data.get_line_alerts(line_num)\n",
    "    if line_alerts:\n",
    "        print(\"‚ö†Ô∏è  ACTIVE ALERTS ON THIS LINE:\")\n",
    "        for alert in line_alerts:\n",
    "            print(f\"   ‚Ä¢ {alert['direction']}: {alert['message']}\")\n",
    "        print()\n",
    "    \n",
    "    # Show northbound\n",
    "    northbound = mta_data.get_line_direction(line_num, 'Northbound')\n",
    "    if northbound:\n",
    "        print(f\"NORTHBOUND: {len(northbound)} trains\")\n",
    "        for train in northbound[:2]:\n",
    "            print()\n",
    "            print(format_train_status(train))\n",
    "        if len(northbound) > 2:\n",
    "            print(f\"\\n   ... and {len(northbound) - 2} more northbound trains\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Show southbound\n",
    "    southbound = mta_data.get_line_direction(line_num, 'Southbound')\n",
    "    if southbound:\n",
    "        print(f\"SOUTHBOUND: {len(southbound)} trains\")\n",
    "        for train in southbound[:2]:\n",
    "            print()\n",
    "            print(format_train_status(train))\n",
    "        if len(southbound) > 2:\n",
    "            print(f\"\\n   ... and {len(southbound) - 2} more southbound trains\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Demo: Show Line 1 and Line 6\n",
    "show_line_status('1')\n",
    "show_line_status('6')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ App data structure ready for development!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === INTEGRATION: Ready for Your Frontend ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ NEXT STEPS FOR YOUR APPLICATION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "YOUR DATA IS READY! \n",
    "\n",
    "The following structures are available:\n",
    "\n",
    "1Ô∏è‚É£  PYTHON OBJECTS (For Backend Processing)\n",
    "   ‚Ä¢ mta_data.trains ‚Üí Dictionary of all trains\n",
    "   ‚Ä¢ mta_data.alerts ‚Üí List of all alerts\n",
    "   ‚Ä¢ mta_data.get_line(line_num) ‚Üí Get trains on a line\n",
    "   ‚Ä¢ mta_data.get_line_direction(line, direction) ‚Üí Get specific trains\n",
    "   ‚Ä¢ mta_data.get_line_alerts(line) ‚Üí Get alerts on a line\n",
    "\n",
    "2Ô∏è‚É£  JSON FILES (For Frontend Integration)\n",
    "   Location: logs/20260202_194910/\n",
    "   Files:\n",
    "   ‚Ä¢ app_data.json ‚Üí Full train + alert data (ready for API)\n",
    "   ‚Ä¢ mta_entities_20260202_194910_fixed.csv ‚Üí Raw data\n",
    "   ‚Ä¢ mta_feed_20260202_194910_fixed.json ‚Üí Original protobuf parse\n",
    "\n",
    "3Ô∏è‚É£  EXAMPLE USAGE IN YOUR APP\n",
    "\n",
    "   # Get Line 1 status\n",
    "   line_1 = mta_data.get_line('1')\n",
    "   \n",
    "   # Filter by direction\n",
    "   northbound = [t for t in line_1 if t['direction'] == 'Northbound']\n",
    "   \n",
    "   # Check for alerts\n",
    "   alerts = mta_data.get_line_alerts('1')\n",
    "   \n",
    "   # Attach alerts to specific train\n",
    "   for train in line_1:\n",
    "       if train['alerts']:\n",
    "           show_warning(f\"Train delayed: {train['alerts'][0]}\")\n",
    "\n",
    "4Ô∏è‚É£  FRONTEND INTEGRATION\n",
    "\n",
    "   a) Load app_data.json in your web/mobile app\n",
    "   \n",
    "   b) Display line status:\n",
    "      ‚îú‚îÄ Show number of trains\n",
    "      ‚îú‚îÄ Show alerts (if any)\n",
    "      ‚îî‚îÄ Show sample trains + delays\n",
    "   \n",
    "   c) Interactive features:\n",
    "      ‚îú‚îÄ Filter by line\n",
    "      ‚îú‚îÄ Filter by direction\n",
    "      ‚îî‚îÄ Show active alerts only\n",
    "\n",
    "5Ô∏è‚É£  API ENDPOINT (If building a backend service)\n",
    "\n",
    "   GET /api/line/1\n",
    "   ‚îî‚îÄ Returns: All trains on Line 1 with alerts\n",
    "   \n",
    "   GET /api/line/1/alerts\n",
    "   ‚îî‚îÄ Returns: All active alerts on Line 1\n",
    "   \n",
    "   GET /api/line/1/Northbound\n",
    "   ‚îî‚îÄ Returns: All northbound trains on Line 1\n",
    "\n",
    "============================================================================\n",
    "\n",
    "STATISTICS FOR YOUR LATEST RUN\n",
    "============================================================================\n",
    "\"\"\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_trains = len(trains)\n",
    "total_alerts = len(alerts)\n",
    "trains_with_alerts = sum(1 for t in trains.values() if t['alerts'])\n",
    "trains_with_delays = sum(1 for t in trains.values() if t['delay_minutes'])\n",
    "\n",
    "print(f\"\"\"\n",
    "Total Entities Parsed:           {total_trains + total_alerts}\n",
    "‚îú‚îÄ Trains:                       {total_trains}\n",
    "‚îî‚îÄ Alerts:                       {total_alerts}\n",
    "\n",
    "Train Status:\n",
    "‚îú‚îÄ On schedule:                  {total_trains - trains_with_delays}\n",
    "‚îî‚îÄ With delays:                  {trains_with_delays}\n",
    "\n",
    "Alert Impact:\n",
    "‚îú‚îÄ Trains with alerts:           {trains_with_alerts}\n",
    "‚îî‚îÄ Trains unaffected:            {total_trains - trains_with_alerts}\n",
    "\n",
    "Data Export:\n",
    "‚îú‚îÄ app_data.json:                {app_data_file}\n",
    "‚îú‚îÄ mta_entities_*_fixed.csv:     {latest_dir}/mta_entities_*_fixed.csv\n",
    "‚îî‚îÄ mta_feed_*_fixed.json:        {latest_dir}/mta_feed_*_fixed.json\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PHASE 2 COMPLETE: Data Processing & App Integration Ready\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "YOU'VE SUCCESSFULLY:\n",
    "  ‚úì Fixed the protobuf parser bugs\n",
    "  ‚úì Extracted real vehicle and alert data\n",
    "  ‚úì Built train objects with route/direction info\n",
    "  ‚úì Attached service alerts to affected trains\n",
    "  ‚úì Created app-ready data structures (JSON + Python objects)\n",
    "  ‚úì Demonstrated rider-facing UI patterns\n",
    "\n",
    "WHAT'S NEXT:\n",
    "  1. Build your frontend (React, Vue, Svelte, etc.)\n",
    "  2. Fetch app_data.json from your backend\n",
    "  3. Render trains by line/direction\n",
    "  4. Show active alerts with warning icons\n",
    "  5. Add real-time updates (re-fetch every 30-60 seconds)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b68ea3",
   "metadata": {},
   "source": [
    "## Understanding the Code - Complete Walkthrough\n",
    "\n",
    "### What This Notebook Does (3 Main Steps)\n",
    "\n",
    "**1. FETCH** ‚Üí Get raw binary data from MTA API\n",
    "```\n",
    "tracker.fetch_data() ‚Üí Makes HTTP request ‚Üí Gets ~200KB protobuf bytes\n",
    "```\n",
    "\n",
    "**2. PARSE** ‚Üí Convert binary protobuf into Python dictionaries  \n",
    "```\n",
    "BetterProtobufParser.parse_feed(data) ‚Üí Decodes binary format ‚Üí Returns {header, entities}\n",
    "```\n",
    "\n",
    "**3. EXPORT** ‚Üí Save parsed data to readable files (JSON, CSV, TXT)\n",
    "```\n",
    "Writes to logs/YYYYMMDD_HHMMSS/ directory with 3 files\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Variable Names & What They Mean\n",
    "\n",
    "**Protobuf Parsing Variables:**\n",
    "- `pos` or `current_position`: Where are we in the binary data? (byte index)\n",
    "- `tag`: Combined field number + wire type\n",
    "- `field_num`: Which field is this? (1=header, 2=entities, etc.)\n",
    "- `wire_type`: How is this field encoded? (0=varint, 2=length-delimited)\n",
    "- `length`: How many bytes does this field contain?\n",
    "- `field_data`: The actual bytes of this field\n",
    "\n",
    "**Entity Variables:**\n",
    "- `entity`: Python dictionary with parsed data from one entity\n",
    "- `entity[\"id\"]`: Unique identifier (e.g., \"000001\")\n",
    "- `entity[\"type\"]`: What kind? (\"vehicle\", \"trip_update\", or \"alert\")\n",
    "- `entity[\"route_id\"]`: Which transit line (e.g., \"A\", \"1\", \"142S\")\n",
    "- `entity[\"trip_id\"]`: Specific trip identifier\n",
    "- `entity[\"alert_message\"]`: Text of alert (or \"Service Alert\" for MTA)\n",
    "- `entity[\"affected_routes\"]`: Which routes are affected\n",
    "\n",
    "**File Export Variables:**\n",
    "- `logs_dir`: Path to \"logs/\" directory\n",
    "- `timestamp`: Current time as string \"20260202_191931\"\n",
    "- `run_dir`: Full path to this run's folder\n",
    "- `csv_file`, `json_file`, `meta_file`: Paths to output files\n",
    "\n",
    "---\n",
    "\n",
    "### Before & After: Alert Data Fix\n",
    "\n",
    "**THE PROBLEM:**\n",
    "```python\n",
    "# Before (broken code was putting route IDs in the wrong column):\n",
    "000009,alert,N/A,N/A,N/A,137S,N/A\n",
    "                            ‚Üë\n",
    "                   Alert_Message column had route data!\n",
    "```\n",
    "\n",
    "**THE ROOT CAUSE:**\n",
    "- MTA protobuf alerts don't have text descriptions\n",
    "- Field 7 contains the route identifier (142S, 103N, etc.)\n",
    "- Old code was treating field 7 as an alert message\n",
    "\n",
    "**THE FIX:**\n",
    "```python\n",
    "# Old extract_alert_info():\n",
    "if field_num == 6:  # header_text\n",
    "    entity[\"alert_message\"] = extract_text()\n",
    "elif field_num == 7:  # description_text\n",
    "    entity[\"alert_message\"] = extract_text()  # ‚Üê WRONG! This is route ID\n",
    "\n",
    "# New extract_alert_info():\n",
    "if field_num == 7:  # field 7 is actually route identifier in MTA data\n",
    "    entity[\"affected_routes\"] = extract_text()  # ‚Üê CORRECT placement\n",
    "    entity[\"alert_message\"] = \"Service Alert\"   # ‚Üê Generic message\n",
    "```\n",
    "\n",
    "**THE RESULT:**\n",
    "```python\n",
    "# After (fixed code puts data in correct columns):\n",
    "000009,alert,N/A,N/A,N/A,Service Alert,137S\n",
    "                             ‚Üë           ‚Üë\n",
    "                    Generic message   Route data (CORRECT!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings About MTA Data\n",
    "\n",
    "| Aspect | Finding | Impact |\n",
    "|--------|---------|--------|\n",
    "| **Alert Text** | ‚ùå Not provided by MTA | All alerts show \"Service Alert\" |\n",
    "| **Route ID** | ‚úÖ In field 7 | Now correctly extracted |\n",
    "| **Vehicle Data** | ‚úÖ Complete (route + trip) | 100% extraction rate |\n",
    "| **Trip Updates** | ‚ùå None available today | Shows 0 trip_updates |\n",
    "| **Parsing Method** | Manual protobuf decoder | No external dependencies needed |\n",
    "\n",
    "---\n",
    "\n",
    "### How Each Parser Method Works\n",
    "\n",
    "**`decode_varint(data, pos)` - Read Variable-Length Integers**\n",
    "```\n",
    "Protobuf numbers aren't fixed-size. Small numbers use fewer bytes.\n",
    "Each byte has:\n",
    "- Bits 0-6: Data bits (7 bits of actual number)\n",
    "- Bit 7: \"More bytes coming\" flag\n",
    "\n",
    "Example:\n",
    "Byte 1: 0b10000001 = keep going, first 7 bits are \"0000001\"\n",
    "Byte 2: 0b00000001 = this is last one, last 7 bits are \"0000001\"\n",
    "Result: 0000001 0000001 = 129\n",
    "```\n",
    "\n",
    "**`parse_feed(data)` - Top Level**\n",
    "```\n",
    "Loop through entire data stream:\n",
    "1. Read tag (field number + wire type)\n",
    "2. If field 1 ‚Üí parse header info\n",
    "3. If field 2 ‚Üí parse one entity, add to list\n",
    "Return {header: {...}, entities: [{...}, {...}, ...]}\n",
    "```\n",
    "\n",
    "**`parse_entity(data)` - One Entity**\n",
    "```\n",
    "For each entity:\n",
    "1. Read ID field (field 1)\n",
    "2. Read type fields (field 2/3/4):\n",
    "   - If field 2 ‚Üí it's a trip_update\n",
    "   - If field 3 ‚Üí it's a vehicle\n",
    "   - If field 4 ‚Üí it's an alert\n",
    "3. Call appropriate extractor based on type\n",
    "Return {id, type, trip_id, route_id, delay, alert_message, affected_routes}\n",
    "```\n",
    "\n",
    "**`extract_vehicle_info(data, entity)` - Vehicle Data**\n",
    "```\n",
    "Vehicles have nested messages:\n",
    "- Field 1: Trip descriptor (contains route_id, trip_id)\n",
    "- Field 2: Position (GPS data - we skip)\n",
    "\n",
    "Call extract_trip_info() to get route + trip\n",
    "```\n",
    "\n",
    "**`extract_trip_info(data, entity)` - Trip Details**\n",
    "```\n",
    "Trip descriptors have:\n",
    "- Field 1: trip_id\n",
    "- Field 3: route_id (note: skips field 2!)\n",
    "\n",
    "Extract both and add to entity\n",
    "```\n",
    "\n",
    "**`extract_alert_info(data, entity)` - Alert Data**\n",
    "```\n",
    "Alerts in MTA data have:\n",
    "- Field 1: Schedule/internal data (we skip)\n",
    "- Field 3-6: Metadata (we skip)\n",
    "- Field 7: Route identifier (e.g., \"142S\", \"103N\")\n",
    "\n",
    "Extract field 7 ‚Üí set affected_routes\n",
    "Set generic message ‚Üí \"Service Alert\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### File Structure After Export\n",
    "\n",
    "```\n",
    "logs/\n",
    "‚îú‚îÄ‚îÄ 20260202_191300/              First run (time-based folder)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ mta_feed_20260202_191300_fixed.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ mta_entities_20260202_191300_fixed.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ mta_metadata_20260202_191300_fixed.txt\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ 20260202_191931/              Second run (later time)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ mta_feed_20260202_191931_fixed.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ mta_entities_20260202_191931_fixed.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ mta_metadata_20260202_191931_fixed.txt\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ ... (more timestamped folders)\n",
    "```\n",
    "\n",
    "Each run is isolated in its own folder so you can compare data across time.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Read the CSV Output\n",
    "\n",
    "**Open the CSV file in Excel or Google Sheets:**\n",
    "\n",
    "```\n",
    "Entity_ID   Type     Route_ID  Trip_ID          Delay  Alert_Message    Affected_Routes\n",
    "--------    ----     --------  -------          -----  ---------------   ----------------\n",
    "000001      vehicle  20260202  106550_1..S03R   N/A    N/A               N/A\n",
    "000002      vehicle  20260202  108950_1..S03R   N/A    N/A               N/A\n",
    "000003      alert    N/A       N/A              N/A    Service Alert     142S\n",
    "000004      vehicle  20260202  109150_1..N03R   N/A    N/A               N/A\n",
    "000005      alert    N/A       N/A              N/A    Service Alert     103N\n",
    "```\n",
    "\n",
    "**Rows:**\n",
    "- Vehicles: Have Route_ID and Trip_ID, rest are N/A\n",
    "- Alerts: Have only Affected_Routes, rest are N/A\n",
    "- Trip_Updates: Would have Route_ID, Trip_ID, and Delay_Seconds\n",
    "\n",
    "**To filter for alerts only in Excel:**\n",
    "1. Click Data ‚Üí Filter\n",
    "2. Click Route_ID dropdown\n",
    "3. Uncheck to show only when empty\n",
    "4. Now you see only the 175 alerts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1714b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def parse_vehicle(data, entity):\n",
    "        \"\"\"Extract vehicle position and trip info\"\"\"\n",
    "        pos = 0\n",
    "        while pos < len(data):\n",
    "            try:\n",
    "                tag, pos = ProtobufParserFixed.decode_varint(data, pos)\n",
    "                field_num = tag >> 3\n",
    "                wire_type = tag & 0x07\n",
    "                \n",
    "                if wire_type == 2:  # Length-delimited\n",
    "                    length, pos = ProtobufParserFixed.decode_varint(data, pos)\n",
    "                    field_data = data[pos:pos+length]\n",
    "                    pos += length\n",
    "                    \n",
    "                    if field_num == 1:  # trip\n",
    "                        ProtobufParserFixed.parse_trip_descriptor(field_data, entity)\n",
    "                    elif field_num == 2:  # position\n",
    "                        # Skip position parsing for now - complex nested format\n",
    "                        pass\n",
    "                elif wire_type == 0:  # Varint\n",
    "                    value, pos = ProtobufParserFixed.decode_varint(data, pos)\n",
    "            except:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfc53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing improved parser with proper error handling...\n",
      "\n",
      "‚úó Error: unpack requires a buffer of 4 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/x6/0kbvm0112w745fhgxxv6wp5c0000gn/T/ipykernel_22940/236673850.py\", line 6, in <module>\n",
      "    feed_fixed = ProtobufParserFixed.parse_feed(tracker.data)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/x6/0kbvm0112w745fhgxxv6wp5c0000gn/T/ipykernel_22940/3356058269.py\", line 45, in parse_feed\n",
      "    entity = ProtobufParserFixed.parse_entity(field_data)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/x6/0kbvm0112w745fhgxxv6wp5c0000gn/T/ipykernel_22940/3356058269.py\", line 108, in parse_entity\n",
      "    ProtobufParserFixed.parse_vehicle(field_data, entity)\n",
      "  File \"/var/folders/x6/0kbvm0112w745fhgxxv6wp5c0000gn/T/ipykernel_22940/3356058269.py\", line 178, in parse_vehicle\n",
      "    ProtobufParserFixed.parse_position(field_data, entity)\n",
      "  File \"/var/folders/x6/0kbvm0112w745fhgxxv6wp5c0000gn/T/ipykernel_22940/3356058269.py\", line 194, in parse_position\n",
      "    value, pos = ProtobufParserFixed.decode_fixed32(data, pos)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/x6/0kbvm0112w745fhgxxv6wp5c0000gn/T/ipykernel_22940/3356058269.py\", line 24, in decode_fixed32\n",
      "    return struct.unpack('<f', data[pos:pos+4])[0], pos + 4\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "struct.error: unpack requires a buffer of 4 bytes\n"
     ]
    }
   ],
   "source": [
    "# Test with wrapped error handling\n",
    "print(\"üîÑ Testing improved parser with proper error handling...\\n\")\n",
    "\n",
    "if tracker.data:\n",
    "    try:\n",
    "        feed_fixed = ProtobufParserFixed.parse_feed(tracker.data)\n",
    "        \n",
    "        print(\"‚úì Successfully parsed with enhanced parser!\\n\")\n",
    "        \n",
    "        # Display header\n",
    "        print(\"üìä Feed Header:\")\n",
    "        if feed_fixed[\"header\"]:\n",
    "            print(f\"   Version: {feed_fixed['header'].get('version', 'N/A')}\")\n",
    "            print(f\"   Timestamp: {feed_fixed['header'].get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Show first 10 entities with details\n",
    "        print(f\"\\nüìã First 10 Entities (with extracted data):\")\n",
    "        print(\"-\" * 110)\n",
    "        for i, e in enumerate(feed_fixed[\"entities\"][:10]):\n",
    "            print(f\"{i+1}. ID: {e['id'][:12]:12} | Type: {e['type']:12} | Route: {e['route_id']:10} | Trip: {e['trip_id'][:12]:12}\")\n",
    "        \n",
    "        # Count populated fields\n",
    "        routes_found = sum(1 for e in feed_fixed['entities'] if e['route_id'] != 'N/A')\n",
    "        trips_found = sum(1 for e in feed_fixed['entities'] if e['trip_id'] != 'N/A')\n",
    "        \n",
    "        print(f\"\\n‚úì Successfully extracted {len(feed_fixed['entities'])} entities\")\n",
    "        print(f\"  - Entities with route_id: {routes_found}\")\n",
    "        print(f\"  - Entities with trip_id: {trips_found}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4911ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEEP DEBUGGING ALERT PROTOBUF STRUCTURE\n",
      "\n",
      "Found Alert #1\n",
      "Raw alert data length: 70 bytes\n",
      "Raw hex (first 100 bytes): 0a360a0e3130383935305f312e2e533033521a0832303236303230322a0131ca3e160a10303120313830392b203234322f53465410011803182628aff884cc063a0431343253\n",
      "\n",
      "Alert Message Fields:\n",
      "----------------------------------------------------------------------\n",
      "  1 (active_period): [binary data, 54 bytes]\n",
      "  7 (description_text): '142S'\n"
     ]
    }
   ],
   "source": [
    "# Debug: Deep dive into alert parsing to understand protobuf structure\n",
    "print(\"üîç DEEP DEBUGGING ALERT PROTOBUF STRUCTURE\\n\")\n",
    "\n",
    "# Let's manually parse the first alert to see what's really in the protobuf\n",
    "if tracker.data:\n",
    "    # Parse and get first alert entity data\n",
    "    data = tracker.data\n",
    "    pos = 0\n",
    "    first_alert_data = None\n",
    "    alert_count = 0\n",
    "    \n",
    "    # Find the first alert in raw feed\n",
    "    while pos < len(data) and alert_count == 0:\n",
    "        tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "        field_num = tag >> 3\n",
    "        wire_type = tag & 0x07\n",
    "        \n",
    "        if wire_type == 2:\n",
    "            length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            field_data = data[pos:pos+length]\n",
    "            pos += length\n",
    "            \n",
    "            if field_num == 2:  # FeedEntity\n",
    "                # Parse this entity to check if it's an alert\n",
    "                entity_pos = 0\n",
    "                is_alert = False\n",
    "                while entity_pos < len(field_data):\n",
    "                    etag, entity_pos = BetterProtobufParser.decode_varint(field_data, entity_pos)\n",
    "                    efield_num = etag >> 3\n",
    "                    ewire_type = etag & 0x07\n",
    "                    \n",
    "                    if ewire_type == 2:\n",
    "                        elength, entity_pos = BetterProtobufParser.decode_varint(field_data, entity_pos)\n",
    "                        edata = field_data[entity_pos:entity_pos+elength]\n",
    "                        entity_pos += elength\n",
    "                        \n",
    "                        if efield_num == 4:  # Alert field\n",
    "                            is_alert = True\n",
    "                            alert_count += 1\n",
    "                            print(f\"Found Alert #{alert_count}\")\n",
    "                            print(f\"Raw alert data length: {len(edata)} bytes\")\n",
    "                            print(f\"Raw hex (first 100 bytes): {edata[:100].hex()}\")\n",
    "                            print()\n",
    "                            \n",
    "                            # Parse this alert message to see all fields\n",
    "                            print(\"Alert Message Fields:\")\n",
    "                            print(\"-\" * 70)\n",
    "                            alert_pos = 0\n",
    "                            field_map = {\n",
    "                                1: \"active_period\",\n",
    "                                2: \"informed_entity\",\n",
    "                                3: \"cause\",\n",
    "                                4: \"effect\", \n",
    "                                5: \"url\",\n",
    "                                6: \"header_text\",\n",
    "                                7: \"description_text\"\n",
    "                            }\n",
    "                            \n",
    "                            while alert_pos < len(edata):\n",
    "                                try:\n",
    "                                    atag, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                    afield_num = atag >> 3\n",
    "                                    awire_type = atag & 0x07\n",
    "                                    \n",
    "                                    field_name = field_map.get(afield_num, f\"field_{afield_num}\")\n",
    "                                    \n",
    "                                    if awire_type == 2:  # Length-delimited\n",
    "                                        alength, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                        avalue = edata[alert_pos:alert_pos+alength]\n",
    "                                        alert_pos += alength\n",
    "                                        \n",
    "                                        if afield_num in [1, 3, 4, 5]:  # Complex types, skip\n",
    "                                            print(f\"  {afield_num} ({field_name}): [binary data, {alength} bytes]\")\n",
    "                                        elif afield_num in [6, 7]:  # Text fields\n",
    "                                            text = avalue.decode('utf-8', errors='ignore')\n",
    "                                            print(f\"  {afield_num} ({field_name}): '{text}'\")\n",
    "                                        elif afield_num == 2:  # informed_entity\n",
    "                                            print(f\"  {afield_num} ({field_name}): [nested message, {alength} bytes]\")\n",
    "                                            # Parse this nested message\n",
    "                                            inform_pos = 0\n",
    "                                            while inform_pos < len(avalue):\n",
    "                                                itag, inform_pos = BetterProtobufParser.decode_varint(avalue, inform_pos)\n",
    "                                                ifield_num = itag >> 3\n",
    "                                                iwire_type = itag & 0x07\n",
    "                                                \n",
    "                                                if iwire_type == 2:\n",
    "                                                    ilength, inform_pos = BetterProtobufParser.decode_varint(avalue, inform_pos)\n",
    "                                                    ivalue = avalue[inform_pos:inform_pos+ilength]\n",
    "                                                    inform_pos += ilength\n",
    "                                                    \n",
    "                                                    ifield_map = {1: \"agency_id\", 2: \"route_id\"}\n",
    "                                                    ifield_name = ifield_map.get(ifield_num, f\"field_{ifield_num}\")\n",
    "                                                    itext = ivalue.decode('utf-8', errors='ignore')\n",
    "                                                    print(f\"       ‚îî‚îÄ {ifield_num} ({ifield_name}): '{itext}'\")\n",
    "                                except:\n",
    "                                    break\n",
    "                            break\n",
    "        elif wire_type == 0:\n",
    "            value, pos = BetterProtobufParser.decode_varint(data, pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301eead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîç PARSING ALERT ACTIVE_PERIOD (Field 1)\n",
      "\n",
      "ALL FIELDS IN FIRST ALERT:\n",
      "  Field 1: \n",
      "\u000e108950_1..S03R20260202*\u00011>\u0016\n",
      "\u001001 1809+ 242/SFT\u0010\u0001\u0018\u0003\n",
      "  Field 3: 38\n",
      "  Field 5: 1770077231\n",
      "  Field 7: 142S\n"
     ]
    }
   ],
   "source": [
    "# Debug: Parse field 1 (active_period) to understand the protobuf structure\n",
    "print(\"\\n\\nüîç PARSING ALERT ACTIVE_PERIOD (Field 1)\\n\")\n",
    "\n",
    "# Re-find the first alert\n",
    "if tracker.data:\n",
    "    data = tracker.data\n",
    "    pos = 0\n",
    "    \n",
    "    while pos < len(data):\n",
    "        tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "        field_num = tag >> 3\n",
    "        wire_type = tag & 0x07\n",
    "        \n",
    "        if wire_type == 2:\n",
    "            length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            field_data = data[pos:pos+length]\n",
    "            pos += length\n",
    "            \n",
    "            if field_num == 2:  # FeedEntity\n",
    "                entity_pos = 0\n",
    "                found_alert = False\n",
    "                while entity_pos < len(field_data):\n",
    "                    etag, entity_pos = BetterProtobufParser.decode_varint(field_data, entity_pos)\n",
    "                    efield_num = etag >> 3\n",
    "                    ewire_type = etag & 0x07\n",
    "                    \n",
    "                    if ewire_type == 2:\n",
    "                        elength, entity_pos = BetterProtobufParser.decode_varint(field_data, entity_pos)\n",
    "                        edata = field_data[entity_pos:entity_pos+elength]\n",
    "                        entity_pos += elength\n",
    "                        \n",
    "                        if efield_num == 4:  # Alert field\n",
    "                            # Parse ALL fields in alert\n",
    "                            alert_pos = 0\n",
    "                            all_fields = []\n",
    "                            \n",
    "                            while alert_pos < len(edata):\n",
    "                                try:\n",
    "                                    atag, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                    afield_num = atag >> 3\n",
    "                                    awire_type = atag & 0x07\n",
    "                                    \n",
    "                                    if awire_type == 2:  # Length-delimited\n",
    "                                        alength, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                        avalue = edata[alert_pos:alert_pos+alength]\n",
    "                                        alert_pos += alength\n",
    "                                        all_fields.append((afield_num, \"LENGTH-DELIMITED\", alength, avalue))\n",
    "                                    elif awire_type == 0:  # Varint\n",
    "                                        avalue, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                        all_fields.append((afield_num, \"VARINT\", avalue, None))\n",
    "                                except:\n",
    "                                    break\n",
    "                            \n",
    "                            print(\"ALL FIELDS IN FIRST ALERT:\")\n",
    "                            for field_num, wtype, val, data_bytes in all_fields:\n",
    "                                if wtype == \"LENGTH-DELIMITED\":\n",
    "                                    try:\n",
    "                                        text = data_bytes.decode('utf-8', errors='ignore')\n",
    "                                        if len(text) < 100:\n",
    "                                            print(f\"  Field {field_num}: {text}\")\n",
    "                                        else:\n",
    "                                            print(f\"  Field {field_num}: {text[:100]}... (truncated)\")\n",
    "                                    except:\n",
    "                                        print(f\"  Field {field_num}: [binary data, {len(data_bytes)} bytes]\")\n",
    "                                else:\n",
    "                                    print(f\"  Field {field_num}: {val}\")\n",
    "                            \n",
    "                            found_alert = True\n",
    "                            break\n",
    "                if found_alert:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c75f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîç CHECKING MULTIPLE ALERTS TO UNDERSTAND PATTERN\n",
      "\n",
      "Alert #1:\n",
      "  Field 1 (text): '\n",
      "\u000e108950_1..S03R20260202*\u00011>\u0016\n",
      "\u001001 1809+ 242/SFT\u0010'\n",
      "  Field 3 (int): 38\n",
      "  Field 5 (int): 1770077231\n",
      "  Field 7 (text): '142S'\n",
      "\n",
      "Alert #2:\n",
      "  Field 1 (text): '\n",
      "\u000e109150_1..N03R20260202*\u00011>\u0014\n",
      "\u001001 1811+ SFT/242\u0010'\n",
      "  Field 3 (int): 37\n",
      "  Field 4 (int): 1\n",
      "  Field 5 (int): 1770077213\n",
      "  Field 7 (text): '103N'\n",
      "\n",
      "Alert #3:\n",
      "  Field 1 (text): '\n",
      "\u000e109350_1..S03R20260202*\u00011>\u0016\n",
      "\u001001 1813+ 242/SFT\u0010'\n",
      "  Field 3 (int): 37\n",
      "  Field 4 (int): 1\n",
      "  Field 5 (int): 1770077244\n",
      "  Field 7 (text): '139S'\n",
      "\n",
      "‚û°Ô∏è  CONCLUSION: Field 7 contains route identifiers, NOT alert descriptions!\n",
      "    Alerts don't have descriptive text - just route identifiers\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check multiple alerts to understand the pattern\n",
    "print(\"\\n\\nüîç CHECKING MULTIPLE ALERTS TO UNDERSTAND PATTERN\\n\")\n",
    "\n",
    "if tracker.data:\n",
    "    data = tracker.data\n",
    "    pos = 0\n",
    "    alert_num = 0\n",
    "    \n",
    "    while pos < len(data) and alert_num < 3:\n",
    "        tag, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "        field_num = tag >> 3\n",
    "        wire_type = tag & 0x07\n",
    "        \n",
    "        if wire_type == 2:\n",
    "            length, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "            field_data = data[pos:pos+length]\n",
    "            pos += length\n",
    "            \n",
    "            if field_num == 2:  # FeedEntity\n",
    "                entity_pos = 0\n",
    "                while entity_pos < len(field_data):\n",
    "                    etag, entity_pos = BetterProtobufParser.decode_varint(field_data, entity_pos)\n",
    "                    efield_num = etag >> 3\n",
    "                    ewire_type = etag & 0x07\n",
    "                    \n",
    "                    if ewire_type == 2:\n",
    "                        elength, entity_pos = BetterProtobufParser.decode_varint(field_data, entity_pos)\n",
    "                        edata = field_data[entity_pos:entity_pos+elength]\n",
    "                        entity_pos += elength\n",
    "                        \n",
    "                        if efield_num == 4:  # Alert field\n",
    "                            alert_num += 1\n",
    "                            print(f\"Alert #{alert_num}:\")\n",
    "                            \n",
    "                            # Parse this alert\n",
    "                            alert_pos = 0\n",
    "                            field_7_value = \"\"\n",
    "                            \n",
    "                            while alert_pos < len(edata):\n",
    "                                try:\n",
    "                                    atag, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                    afield_num = atag >> 3\n",
    "                                    awire_type = atag & 0x07\n",
    "                                    \n",
    "                                    if awire_type == 2:\n",
    "                                        alength, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                        avalue = edata[alert_pos:alert_pos+alength]\n",
    "                                        alert_pos += alength\n",
    "                                        try:\n",
    "                                            text = avalue.decode('utf-8', errors='ignore')\n",
    "                                            print(f\"  Field {afield_num} (text): '{text[:50]}'\")\n",
    "                                            if afield_num == 7:\n",
    "                                                field_7_value = text\n",
    "                                        except:\n",
    "                                            print(f\"  Field {afield_num}: [binary, {alength} bytes]\")\n",
    "                                    elif awire_type == 0:\n",
    "                                        avalue, alert_pos = BetterProtobufParser.decode_varint(edata, alert_pos)\n",
    "                                        print(f\"  Field {afield_num} (int): {avalue}\")\n",
    "                                except:\n",
    "                                    break\n",
    "                            print()\n",
    "        elif wire_type == 0:\n",
    "            value, pos = BetterProtobufParser.decode_varint(data, pos)\n",
    "\n",
    "print(\"‚û°Ô∏è  CONCLUSION: Field 7 contains route identifiers, NOT alert descriptions!\")\n",
    "print(\"    Alerts don't have descriptive text - just route identifiers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc3915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 19:01:44,539 - INFO - Session closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Session closed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Close the tracker session\n",
    "tracker.close()\n",
    "print(\"‚úì Session closed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf89b8",
   "metadata": {},
   "source": [
    "## Step 7: Save and Export Data\n",
    "\n",
    "Let's save the parsed MTA data to files you can download and analyze later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db10f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving MTA data to files...\n",
      "\n",
      "‚úì Saved: mta_feed_20260202_190144.json\n",
      "‚úì Saved: mta_entities_20260202_190144.csv\n",
      "‚úì Saved: mta_metadata_20260202_190144.txt\n",
      "\n",
      "üìÅ Files saved to current directory:\n",
      "   1Ô∏è‚É£  mta_feed_20260202_190144.json\n",
      "       ‚îî‚îÄ Full data (JSON format)\n",
      "   2Ô∏è‚É£  mta_entities_20260202_190144.csv\n",
      "       ‚îî‚îÄ Entities (CSV - open in Excel)\n",
      "   3Ô∏è‚É£  mta_metadata_20260202_190144.txt\n",
      "       ‚îî‚îÄ Legend & data dictionary\n",
      "\n",
      "üí° Start with the metadata file to understand the data!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üíæ Saving MTA data to files...\\n\")\n",
    "\n",
    "if tracker.data and 'feed' in locals():\n",
    "    # Create timestamp for filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Save full feed as JSON\n",
    "    json_file = f\"mta_feed_{timestamp}.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(feed, f, indent=2, default=str)\n",
    "    print(f\"‚úì Saved: {json_file}\")\n",
    "    \n",
    "    # 2. Save entities as CSV\n",
    "    csv_file = f\"mta_entities_{timestamp}.csv\"\n",
    "    if feed[\"entities\"]:\n",
    "        with open(csv_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Entity_ID\", \"Type\", \"Route\", \"Trip_ID\", \"Delay_Seconds\", \"Latitude\", \"Longitude\"])\n",
    "            \n",
    "            for entity in feed[\"entities\"]:\n",
    "                entity_id = entity.get(\"id\", \"N/A\")[:50]\n",
    "                ent_type = entity.get(\"type\", \"unknown\")\n",
    "                route = entity.get(\"data\", {}).get(\"route\", \"N/A\")\n",
    "                trip = entity.get(\"data\", {}).get(\"trip\", \"N/A\")\n",
    "                delay = entity.get(\"data\", {}).get(\"delay\", \"N/A\")\n",
    "                lat = entity.get(\"data\", {}).get(\"latitude\", \"N/A\")\n",
    "                lon = entity.get(\"data\", {}).get(\"longitude\", \"N/A\")\n",
    "                \n",
    "                writer.writerow([entity_id, ent_type, route, trip, delay, lat, lon])\n",
    "    \n",
    "    print(f\"‚úì Saved: {csv_file}\")\n",
    "    \n",
    "    # 3. Save metadata as text\n",
    "    meta_file = f\"mta_metadata_{timestamp}.txt\"\n",
    "    with open(meta_file, 'w') as f:\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(\"MTA GTFS-REALTIME DATA EXPORT\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"EXPORT TIMESTAMP: \" + datetime.now().isoformat() + \"\\n\")\n",
    "        f.write(\"DATA SIZE: \" + f\"{len(tracker.data):,} bytes\\n\\n\")\n",
    "        \n",
    "        f.write(\"FEED HEADER INFORMATION:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        if feed[\"header\"]:\n",
    "            f.write(f\"GTFS Version: {feed['header'].get('version', 'N/A')}\\n\")\n",
    "            f.write(f\"Feed Timestamp: {feed['header'].get('timestamp', 'N/A')}\\n\")\n",
    "            f.write(f\"Incrementality: {feed['header'].get('incrementality', 'N/A')}\\n\")\n",
    "        \n",
    "        f.write(\"\\nENTITY BREAKDOWN:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        trip_updates = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"trip_update\")\n",
    "        vehicles = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"vehicle\")\n",
    "        alerts = sum(1 for e in feed[\"entities\"] if e[\"type\"] == \"alert\")\n",
    "        f.write(f\"Total Entities: {len(feed['entities'])}\\n\")\n",
    "        f.write(f\"  - Trip Updates: {trip_updates}\\n\")\n",
    "        f.write(f\"  - Vehicle Positions: {vehicles}\\n\")\n",
    "        f.write(f\"  - Service Alerts: {alerts}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(\"DATA LEGEND / CYPHER\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"FILE DESCRIPTIONS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"1. {json_file}\\n\")\n",
    "        f.write(\"   Full structured JSON export of all feed data\\n\")\n",
    "        f.write(\"   Format: {header: {...}, entities: [...]}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"2. {csv_file}\\n\")\n",
    "        f.write(\"   Comma-separated entities with key information\\n\")\n",
    "        f.write(\"   Easy to open in Excel or Google Sheets\\n\\n\")\n",
    "        \n",
    "        f.write(\"FIELD DEFINITIONS:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"Entity_ID: Unique identifier for this transit entity\\n\")\n",
    "        f.write(\"Type: Entity type (trip_update, vehicle, alert)\\n\")\n",
    "        f.write(\"Route: MTA route ID (e.g., '1', 'A', 'F')\\n\")\n",
    "        f.write(\"Trip_ID: Unique trip identifier\\n\")\n",
    "        f.write(\"Delay_Seconds: Delay in seconds (for trip updates)\\n\")\n",
    "        f.write(\"Latitude: Vehicle latitude (for vehicle positions)\\n\")\n",
    "        f.write(\"Longitude: Vehicle longitude (for vehicle positions)\\n\\n\")\n",
    "        \n",
    "        f.write(\"ENTITY TYPES EXPLAINED:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"trip_update:\\n\")\n",
    "        f.write(\"  - Real-time updates about scheduled trips\\n\")\n",
    "        f.write(\"  - Includes: Route ID, delay information, stop updates\\n\")\n",
    "        f.write(\"  - Use: Track schedule changes and delays\\n\\n\")\n",
    "        \n",
    "        f.write(\"vehicle:\\n\")\n",
    "        f.write(\"  - Real-time location of transit vehicles\\n\")\n",
    "        f.write(\"  - Includes: Route, latitude, longitude, bearing\\n\")\n",
    "        f.write(\"  - Use: Track vehicle positions on map\\n\\n\")\n",
    "        \n",
    "        f.write(\"alert:\\n\")\n",
    "        f.write(\"  - Service alerts and notifications\\n\")\n",
    "        f.write(\"  - Includes: Alert messages, affected routes\\n\")\n",
    "        f.write(\"  - Use: Inform users of service changes\\n\\n\")\n",
    "        \n",
    "        f.write(\"HOW TO USE THIS DATA:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"1. Open {csv_file} in Excel/Google Sheets for quick overview\\n\")\n",
    "        f.write(f\"2. Use {json_file} for programmatic access to full data\\n\")\n",
    "        f.write(\"3. Refer to this file for field explanations\\n\\n\")\n",
    "        \n",
    "        f.write(\"EXAMPLE QUERIES:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"Find delayed trips:\\n\")\n",
    "        f.write(\"  - Open CSV, filter Delay_Seconds > 0\\n\\n\")\n",
    "        \n",
    "        f.write(\"Track a specific route (e.g., 'A' line):\\n\")\n",
    "        f.write(\"  - CSV: Filter Route column = 'A'\\n\")\n",
    "        f.write(\"  - JSON: Search for entities with route_id: 'A'\\n\\n\")\n",
    "        \n",
    "        f.write(\"Map vehicle locations:\\n\")\n",
    "        f.write(\"  - Use Latitude + Longitude columns in Google Maps\\n\")\n",
    "        f.write(\"  - Plot as custom locations\\n\\n\")\n",
    "        \n",
    "        f.write(\"NOTES:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(\"- N/A indicates missing or unavailable data\\n\")\n",
    "        f.write(\"- Timestamps are Unix epoch format (seconds since 1970)\\n\")\n",
    "        f.write(\"- Coordinates use WGS84 (standard GPS)\\n\")\n",
    "        f.write(\"- Data is real-time and changes every 30-60 seconds\\n\")\n",
    "        f.write(\"- Files are timestamped for archival and comparison\\n\")\n",
    "    \n",
    "    print(f\"‚úì Saved: {meta_file}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Files saved to current directory:\")\n",
    "    print(f\"   1Ô∏è‚É£  {json_file}\")\n",
    "    print(f\"       ‚îî‚îÄ Full data (JSON format)\")\n",
    "    print(f\"   2Ô∏è‚É£  {csv_file}\")\n",
    "    print(f\"       ‚îî‚îÄ Entities (CSV - open in Excel)\")\n",
    "    print(f\"   3Ô∏è‚É£  {meta_file}\")\n",
    "    print(f\"       ‚îî‚îÄ Legend & data dictionary\")\n",
    "    print(f\"\\nüí° Start with the metadata file to understand the data!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö† No parsed data available to save.\")\n",
    "    print(\"   Run the parsing cell first to generate feed data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
